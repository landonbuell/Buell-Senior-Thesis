\relax 
\citation{Khan}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}\protected@file@percent }
\newlabel{sec-Introduction}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Methodology}{3}\protected@file@percent }
\newlabel{sec-Methodology}{{1.1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Designing the Function}{3}\protected@file@percent }
\newlabel{eqn-MappingFunction}{{1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Collecting and Pre-processing Raw Data}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Designing Classification Features}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4}Designing A Complementary Network Architecture}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.5}Testing and Evaluating Network Performance}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.6}Running Predictions of Chaotic Synthesizer Files}{4}\protected@file@percent }
\citation{Bishop}
\citation{Mitchell}
\citation{Geron2}
\citation{Goodfellow}
\citation{Levine}
\citation{Geron}
\citation{Geron}
\citation{Levine}
\citation{Bishop}
\citation{Geron}
\citation{Virtanen}
\citation{White}
\citation{Olson}
\citation{Olson}
\citation{Goodfellow}
\citation{James}
\citation{Virtanen}
\citation{Goodfellow}
\citation{Loy}
\citation{Virtanen}
\@writefile{toc}{\contentsline {section}{\numberline {2}The Neural Network}{5}\protected@file@percent }
\newlabel{sec-TheNeuralNetwork}{{2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}An Introduction to Neural Networks}{5}\protected@file@percent }
\newlabel{subsec-NerualNetworkIntro}{{2.1}{5}}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\citation{Geron}
\citation{Loy}
\citation{Geron}
\citation{James}
\citation{Loy}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Structure of a Neural Network}{6}\protected@file@percent }
\newlabel{subsec-NetworkStructure}{{2.2}{6}}
\newlabel{eqn-FunctionGraph}{{2}{6}}
\newlabel{eqn-FunctionChain}{{3}{6}}
\newlabel{eqn-altLayerFunction}{{4}{6}}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward propagation system in a standard deep neural network. Each layer is presumed to be a node in a linked computational graph. This example has been setup to assume one input layer, and one output layer. Practical implementations should include mini-batches of data as opposed to a single sample.\relax }}{7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg-FeedForward}{{1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Layers Used in this Classification Model}{7}\protected@file@percent }
\newlabel{subsec-Layers}{{2.3}{7}}
\newlabel{eqn-LinearTransform}{{5}{7}}
\newlabel{eqn-LinearTransform2}{{6}{7}}
\citation{Geron2}
\citation{Loy}
\citation{McCulloch}
\citation{McCulloch}
\citation{Geron}
\citation{Loy}
\citation{Levine}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Tensorflow}
\newlabel{eqn-elementActivation}{{7}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Dense Layer}{8}\protected@file@percent }
\newlabel{subsubsec-DenseLayer}{{2.3.1}{8}}
\newlabel{layer-DenseNeurons}{{8}{8}}
\newlabel{eqn-FunctionDense}{{9}{8}}
\newlabel{eqn-DenseFeedForward}{{11}{8}}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Typical "Call" method for a dense layer in a neural network that contains $n$ neurons/nodes. This example shows the computation over a single input $x^{(l-1)}$ but a practical implementation should include mini-batches of samples.\relax }}{9}\protected@file@percent }
\newlabel{alg-CallDense}{{2}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}2-Dimensional Convolution Layer}{9}\protected@file@percent }
\newlabel{subsubsec-Conv2DLayer}{{2.3.2}{9}}
\newlabel{eqn-convolution}{{12}{9}}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The result of convolving an input (a) with an filter map (b) is a new set of activations (c). This Image was adapted from Goodfellow, pg. 325 \cite  {Goodfellow}\relax }}{10}\protected@file@percent }
\newlabel{fig-2DConvExample}{{1}{10}}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Loy}
\newlabel{eqn-ConvFeedForward}{{14}{11}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Typical "Call" method for a 2-Dimensional Convolutional layer in a neural network that uses $K$ filters, of $n \times m$ kernels, with an assumed stride size of $1 \times 1$. This example shows the computation over a single input $x^{(l-1)}$ but a practical implementation should include mini-batches of samples.\relax }}{11}\protected@file@percent }
\newlabel{alg-CallConv2D}{{3}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}2-Dimensional Maximum Pooling Layer}{11}\protected@file@percent }
\newlabel{subsubsec-2DPool}{{2.3.3}{11}}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The result of 2D maximum-pooling an input array. This image was adapted from Loy, pg. 126 \cite  {Loy}\relax }}{12}\protected@file@percent }
\newlabel{fig-2DMaxPool}{{2}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}1-Dimensional Flattening Layer}{12}\protected@file@percent }
\newlabel{subsubsec-1DFlatten}{{2.3.4}{12}}
\newlabel{eqn-FlattenFunction}{{15}{12}}
\citation{Geron}
\citation{Loy}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}1-Dimensional Concatenation Layer}{13}\protected@file@percent }
\newlabel{subsubsec-1DConcat}{{2.3.5}{13}}
\newlabel{eqn-ConcatenationFunction}{{18}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Activation Functions Used in Network Layers}{13}\protected@file@percent }
\newlabel{sec-ActivationFunctions}{{2.4}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Rectified Linear Unit}{13}\protected@file@percent }
\newlabel{eqn-ReLU}{{19}{13}}
\citation{Geron}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Loy}
\citation{Virtanen}
\citation{Goodfellow}
\citation{Mitchell}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Rectifed Linear Unit (ReLU) activation function\relax }}{14}\protected@file@percent }
\newlabel{fig-ReLU}{{3}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Softmax}{14}\protected@file@percent }
\newlabel{subsubsec-Softmax}{{2.4.2}{14}}
\newlabel{eqn-Softmax}{{20}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Training The Model}{14}\protected@file@percent }
\newlabel{subsec-Training}{{2.5}{14}}
\citation{Geron}
\citation{Goodfellow}
\citation{Levine}
\citation{Goodfellow}
\citation{James}
\citation{James}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Bishop}
\newlabel{eqn-Theta}{{21}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}The Cost Function}{15}\protected@file@percent }
\newlabel{eqn-CXELoss}{{22}{15}}
\newlabel{eqn-CXELossAvg}{{23}{15}}
\citation{Goodfellow}
\citation{Loy}
\citation{Mitchell}
\citation{James}
\citation{Loy}
\citation{Goodfellow}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plot of how $y^*_{j}$ affects the output values of the CXE cost function\relax }}{16}\protected@file@percent }
\newlabel{fig-CXELoss}{{4}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Gradient Based Learning}{16}\protected@file@percent }
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\newlabel{eqn-CostGradient}{{25}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Back-Propagation}{17}\protected@file@percent }
\newlabel{subsubsec-BackProp}{{2.5.3}{17}}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A visual representation of a simplified neural network as a computational graph\relax }}{19}\protected@file@percent }
\newlabel{fig-ComputationalGraph}{{5}{19}}
\newlabel{eqn-dWL1}{{27}{20}}
\newlabel{eqn-dbL1}{{28}{20}}
\newlabel{eqn-CXELossDeriv}{{29}{20}}
\citation{Geron2}
\citation{Goodfellow}
\citation{Geron}
\newlabel{eqn-dWGeneral}{{34}{21}}
\newlabel{eqn-dbGeneral}{{35}{21}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Backwards propagation system, in a standard densely connected deep neural network. Each iteration in the \textit  {for-loop} computes the gradient of the cost function $J$ with respect to the weight and bias arrays in a given layer $(l)$. Each element in those arrays $dW$ and $db$ is the discrete gradient of the cost due to that parameter. A practical application of this algorithm should include batches of samples instead of a single sample and a regularizing function at each step.\relax }}{21}\protected@file@percent }
\newlabel{alg-BackProp}{{4}{21}}
\citation{Geron}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Geron}
\newlabel{eqn-GradientLearning}{{36}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.4}The Optimizer}{22}\protected@file@percent }
\newlabel{eqn-ADAMupdate}{{37}{22}}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Geron}
\citation{Goodfellow}
\citation{Ngiam}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Adaptive-Moments (ADAM) optimizer for a neural network. This algorithm is adapted from Goodfellow, \cite  {Goodfellow}\relax }}{23}\protected@file@percent }
\newlabel{alg-ADAM}{{5}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Multimodal Architecture}{23}\protected@file@percent }
\newlabel{subsec-Architecture}{{2.6}{23}}
\citation{Li}
\citation{Ngiam}
\citation{White}
\citation{Olson}
\citation{Khan}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The implemented miltimodal architecture of the audio file classification neural network. The Left branch process an image-like input, the right branch processes a vector-like input. The activations are then merged, and then a single output vector is produced\relax }}{25}\protected@file@percent }
\newlabel{fig-NetworkArchitecture}{{6}{25}}
\citation{Goodfellow}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Khan}
\citation{Serizel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}The Spectrogram Branch}{26}\protected@file@percent }
\newlabel{eqn-shapeX1}{{38}{26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}The Perceptron Branch}{26}\protected@file@percent }
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{James}
\newlabel{eqn-shapeX2}{{39}{27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}The Final Output Branch}{27}\protected@file@percent }
\citation{Virtanen}
\citation{Liu}
\citation{James}
\citation{Serizel}
\@writefile{toc}{\contentsline {section}{\numberline {3}Properties of Musical Instruments}{28}\protected@file@percent }
\newlabel{sec-Instruments}{{3}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Instruments categories used in the classification task. Note that the network uses only integers given by "Class index" to indentify sources. The string "Class Name" is keep for human readability\relax }}{28}\protected@file@percent }
\newlabel{fig-ClassList}{{7}{28}}
\citation{Haberman}
\citation{Hunter}
\citation{Taylor}
\citation{Olson}
\citation{White}
\citation{Haberman}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Idiophones}{29}\protected@file@percent }
\newlabel{subsec-Idiophone}{{3.1}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Membranophones}{29}\protected@file@percent }
\newlabel{subsec-membranophones}{{3.2}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Chordophones}{29}\protected@file@percent }
\newlabel{subsec-chordophones}{{3.3}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}The 1-Dimensional Vibrating String}{29}\protected@file@percent }
\newlabel{eqn-1DWaveEqn}{{40}{29}}
\citation{Hunter}
\citation{Taylor}
\citation{White}
\citation{Haberman}
\citation{Hunter}
\citation{Olson}
\citation{Taylor}
\citation{White}
\citation{White}
\citation{White}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Aerophones}{30}\protected@file@percent }
\newlabel{subsec-Aerophones}{{3.4}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Other Generated Sounds}{30}\protected@file@percent }
\newlabel{subsec-Generated}{{3.5}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Sine Wave}{30}\protected@file@percent }
\citation{White}
\citation{Olson}
\citation{White}
\newlabel{eqn-SineWave}{{43}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A sine wave in the (a) time-domian and (b) frequency domain\relax }}{31}\protected@file@percent }
\newlabel{fig-SineWave}{{9}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Sawtooth Wave}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}Square Wave}{31}\protected@file@percent }
\newlabel{eqn-SquareWave}{{44}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A square wave in the (a) time-domain and (b) frequency domain\relax }}{31}\protected@file@percent }
\newlabel{fig-SquareWave}{{10}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.4}Triangle Wave}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.5}White Noise}{31}\protected@file@percent }
\citation{James}
\citation{Loy}
\citation{Serizel}
\citation{Geron}
\citation{James}
\citation{Khan}
\citation{Mierswa}
\citation{Serizel}
\citation{Liu}
\citation{Olson}
\citation{Virtanen}
\citation{Liu}
\citation{Goodfellow}
\citation{James}
\citation{Serizel}
\citation{Goodfellow}
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\@writefile{toc}{\contentsline {section}{\numberline {4}Feature Selections}{32}\protected@file@percent }
\newlabel{sec-Features}{{4}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Feature Space}{32}\protected@file@percent }
\citation{Virtanen}
\citation{Goodfellow}
\citation{James}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{James}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Decision Boundaries}{33}\protected@file@percent }
\newlabel{subsubsec-Decision}{{4.1.1}{33}}
\citation{James}
\citation{Loy}
\citation{Geron}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Feature Space with Linear separability between two classes\relax }}{34}\protected@file@percent }
\newlabel{fig-LinSep1}{{11}{34}}
\citation{Bishop}
\citation{James}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Feature Space with near linear separability between two classes\relax }}{35}\protected@file@percent }
\newlabel{fig-LinSep2}{{12}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Feature Space with near linear non-separability between two classes\relax }}{35}\protected@file@percent }
\newlabel{fig-LinNonSep}{{13}{35}}
\citation{Goodfellow}
\citation{James}
\citation{Bishop}
\citation{Geron}
\citation{Li}
\citation{Ngiam}
\citation{Tensorflow}
\citation{Loy}
\citation{Geron2}
\citation{James}
\citation{Serizel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}The Design Matrix}{36}\protected@file@percent }
\newlabel{subsubsec-DesignMatrix}{{4.1.2}{36}}
\newlabel{eqn-X1Shape}{{46}{36}}
\newlabel{eqn-X2Shape}{{47}{36}}
\newlabel{eqn-YShape}{{48}{36}}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Audio Preprocessing}{37}\protected@file@percent }
\newlabel{subsubsec-Preprocessing}{{4.1.3}{37}}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\citation{Mierswa}
\citation{Liu}
\citation{Zhang}
\citation{Kahn}
\citation{Serizel}
\citation{Liu}
\citation{Liu}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Spectrogram Features}{38}\protected@file@percent }
\newlabel{subsec-Spectrogram}{{4.2}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Spectrogram representations of various waveforms\relax }}{38}\protected@file@percent }
\newlabel{fig-spectrograms}{{14}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Frame Blocking}{38}\protected@file@percent }
\newlabel{subsubsec-FrameBlocking}{{4.2.1}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces A visualization of how frame-blocking is used to create each analysis frames. This image has been adapted and modified from Liu, et. al. "Audio Feature Extraction and Analysis", Fig. (1). See ref. \cite  {Liu}.\relax }}{38}\protected@file@percent }
\newlabel{fig-AnalysisFrames}{{15}{38}}
\newlabel{eqn-FrameMatrix}{{49}{39}}
\newlabel{eqn-IndexingA}{{50}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Windowing}{39}\protected@file@percent }
\newlabel{subsubsec-Windowing}{{4.2.2}{39}}
\newlabel{eqn-Hanning}{{51}{39}}
\newlabel{eqn-WindowMatrix}{{52}{39}}
\citation{Virtanen}
\citation{Olson}
\citation{Peatross}
\citation{Virtanen}
\citation{Taylor}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Discrete Fourier Transform}{40}\protected@file@percent }
\newlabel{subsubsec-DFT}{{4.2.3}{40}}
\citation{Short}
\citation{Peatross}
\citation{Levine}
\citation{Loy}
\citation{Tensorflow}
\citation{Olson}
\citation{Virtanen}
\citation{White}
\newlabel{eqn-DFTMatrix}{{53}{41}}
\newlabel{eqn-DFT}{{54}{41}}
\newlabel{eqn-Spectrogram}{{55}{41}}
\newlabel{eqn-IndexingS}{{56}{41}}
\newlabel{eqn-X1}{{57}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Time-Space Features}{42}\protected@file@percent }
\newlabel{subsec-TimeFeatures}{{4.3}{42}}
\citation{Serizel}
\citation{Liu}
\citation{Olson}
\citation{Serizel}
\citation{Virtanen}
\citation{Olson}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Time Domain Envelope}{43}\protected@file@percent }
\newlabel{eqn-RMS}{{58}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces TDE Envelope values for musical instruments\relax }}{43}\protected@file@percent }
\newlabel{fig-ExampleTDE}{{17}{43}}
\citation{Khan}
\citation{Liu}
\citation{Zhang}
\citation{Serizel}
\citation{Liu}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces A comparison of the first $3$ of $5$ Time Domain Envelope across each class using a box-and-whisker plot\relax }}{45}\protected@file@percent }
\newlabel{fig-FeatureTDE}{{18}{45}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Zero Crossing Rate}{45}\protected@file@percent }
\citation{Liu}
\citation{White}
\citation{Loy}
\citation{James}
\newlabel{eqn-ZXR}{{59}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces A comparison of the Zero-Crossing Rate for each class using a box-and-whisker plot\relax }}{46}\protected@file@percent }
\newlabel{fig-FeatureZXR}{{19}{46}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Temporal Center of Mass}{46}\protected@file@percent }
\citation{Olson}
\citation{White}
\citation{Serizel}
\newlabel{eqn-FeatureTCM}{{60}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces TCM values for musical instruments\relax }}{47}\protected@file@percent }
\newlabel{fig-ExampleTCM}{{20}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces A comparison of the temporal center mass for each class using a box-and-whisker plot\relax }}{47}\protected@file@percent }
\newlabel{fig-FeatureTCM}{{21}{47}}
\citation{Serizel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Auto Correlation Coefficients}{48}\protected@file@percent }
\newlabel{eqn-FeatureACC}{{61}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces A comparison of the the first four auto correlation coefficients in each class using a box-and-whisker plot\relax }}{49}\protected@file@percent }
\newlabel{fig-FeatureACC}{{22}{49}}
\citation{Sahidullah}
\citation{Serizel}
\citation{Sahidullah}
\citation{Serizel}
\citation{Serizel}
\citation{Khan}
\citation{Serizel}
\citation{Kahn}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Frequency-Space Features}{50}\protected@file@percent }
\newlabel{subsec-FreqFeatures}{{4.4}{50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Mel Filter Bank Energies}{50}\protected@file@percent }
\newlabel{eqn-HztoMel}{{62}{50}}
\newlabel{eqn-MeltoHz}{{63}{50}}
\citation{Serizel}
\citation{Sahidullah}
\citation{Virtanen}
\citation{Serizel}
\citation{Sahidullah}
\citation{Liu}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Mel Filter Banks shown in frequency space with units of Hertz\relax }}{51}\protected@file@percent }
\newlabel{fig-MelFilterBanks}{{23}{51}}
\newlabel{eqn-FilterBanks}{{64}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Mel Frequency Cepstral Coeffecients}{51}\protected@file@percent }
\newlabel{feat-MFCC}{{65}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces A comparison of 4 Mel Frequency Cepstral Coefficients, $1$, $4$, $8$ and $12$ for each class using box-and-whisker plots\relax }}{53}\protected@file@percent }
\newlabel{fig-FeatureMFCCs}{{24}{53}}
\citation{Olson}
\citation{White}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Frequency Center of Mass}{54}\protected@file@percent }
\newlabel{eqn-FeatureFCM}{{66}{54}}
\newlabel{fig-FeatureFCM}{{\caption@xref {fig-FeatureFCM}{ on input line 1581}}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces A comparison of the frequency center-of-mass for each class using box-and-whisker plots\relax }}{54}\protected@file@percent }
\citation{Geron}
\citation{Goodfellow}
\citation{Geron2}
\citation{Mitchell}
\citation{Geron}
\citation{Geron}
\citation{James}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluating Model Performance}{55}\protected@file@percent }
\newlabel{sec-PerfEval}{{5}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}K-Folds Cross Validation}{55}\protected@file@percent }
\newlabel{subsec-XValidation}{{5.1}{55}}
\newlabel{eqn-XValSplit}{{68}{55}}
\citation{James}
\citation{Geron}
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces A $K$-Fold Cross Validation program.\relax }}{56}\protected@file@percent }
\newlabel{alg-CrossValidation}{{6}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Performance Metrics}{56}\protected@file@percent }
\citation{Geron}
\citation{Geron}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Confusion Matrix}{57}\protected@file@percent }
\newlabel{eqn-ConfMat}{{5.2.1}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Example Confusion Matrices for $4$-categories classifier\relax }}{57}\protected@file@percent }
\newlabel{fig-DummyConfMat}{{26}{57}}
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{James}
\citation{Geron}
\citation{James}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Precision Score}{58}\protected@file@percent }
\newlabel{eqn-BinaryPrecision}{{69}{58}}
\newlabel{eqn-KPrecision}{{70}{58}}
\citation{Geron}
\citation{James}
\citation{Geron}
\citation{Geron}
\citation{James}
\citation{Geron}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Recall Score}{59}\protected@file@percent }
\newlabel{eqn-BinaryRecall}{{71}{59}}
\newlabel{eqn-KRecall}{{72}{59}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}F1-Score}{59}\protected@file@percent }
\newlabel{eqn-F1Score}{{73}{59}}
\citation{James}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{James}
\citation{Loy}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.5}Accuracy Score}{60}\protected@file@percent }
\newlabel{eqn-BinaryAccuracy}{{74}{60}}
\newlabel{eqn-Accuracy}{{75}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Tracking Metrics over a Period of Training}{60}\protected@file@percent }
\newlabel{subsec-TrainingMetrics}{{5.3}{60}}
\citation{Geron}
\citation{James}
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\citation{Geron}
\@writefile{toc}{\contentsline {paragraph}{}{61}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces The loss function score decreases with each training step, indicating that optimization is performing correctly\relax }}{61}\protected@file@percent }
\newlabel{fig-LossScore}{{27}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces The precision score (a) and recall score (b) increases with each training step\relax }}{61}\protected@file@percent }
\newlabel{fig-PrecisionRecallScores}{{28}{61}}
\citation{James}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Boosted Aggregation}{62}\protected@file@percent }
\newlabel{subsec-Bagging}{{5.4}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experimental Results}{63}\protected@file@percent }
\newlabel{sec-Results}{{6}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Executing Cross Validation}{63}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Metrics averaged over all classes for $K=10$ validation splits\relax }}{63}\protected@file@percent }
\newlabel{fig-ValidationTable}{{29}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Metric statisitcs over all classes and all validation splits\relax }}{63}\protected@file@percent }
\newlabel{fig-ValidationStats}{{30}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Performance Metrics Across all High Ranged Woodwind Instruments\relax }}{64}\protected@file@percent }
\newlabel{fig-HighWindsScores}{{31}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Performance Metrics Across all Mid and Low Ranged Woodwind Instruments\relax }}{65}\protected@file@percent }
\newlabel{fig-LowWindsScores}{{32}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Performance Metrics Across all Brass Instruments\relax }}{66}\protected@file@percent }
\newlabel{fig-BrassScores}{{33}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}\IeC {\textbullet }}{66}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{67}\protected@file@percent }
\newlabel{sec-Conclusion}{{7}{67}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgments}{68}\protected@file@percent }
\newlabel{sec-Acknowledge}{{8}{68}}
\bibstyle{apalike}
\bibcite{Bishop}{1}
\bibcite{Geron}{2}
\bibcite{Geron2}{3}
\bibcite{Goodfellow}{4}
\bibcite{Haberman}{5}
\bibcite{Hunter}{6}
\bibcite{James}{7}
\bibcite{Khan}{8}
\bibcite{Levine}{9}
\bibcite{Li}{10}
\bibcite{Liu}{11}
\bibcite{Loy}{12}
\bibcite{McCulloch}{13}
\bibcite{Mierswa}{14}
\bibcite{Mitchell}{15}
\bibcite{Ngiam}{16}
\bibcite{Olson}{17}
\bibcite{Peatross}{18}
\bibcite{Petrik}{19}
\bibcite{Powers}{20}
\bibcite{Sahidullah}{21}
\bibcite{Serizel}{22}
\bibcite{Short}{23}
\bibcite{Taylor}{24}
\bibcite{Tensorflow}{25}
\bibcite{Virtanen}{26}
\bibcite{White}{27}
\bibcite{Zhang}{28}
