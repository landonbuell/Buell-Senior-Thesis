\relax 
\citation{Khan}
\citation{Mierswa}
\citation{Liu}
\citation{Zhang}
\citation{Bishop}
\citation{Geron}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Levine}
\citation{Loy}
\citation{Khan}
\citation{Liu}
\citation{Mierswa}
\citation{Serizel}
\citation{Zhang}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}\protected@file@percent }
\newlabel{sec-Introduction}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Introduction}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Methodology}{4}\protected@file@percent }
\newlabel{sec-Methodology}{{1.2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Designing the Function}{4}\protected@file@percent }
\newlabel{eqn-MappingFunction}{{1}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Collecting and Pre-processing Raw Data}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Designing Classification Features}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.4}Designing A Complementary Network Architecture}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.5}Testing and Evaluating Network Performance}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.6}Running Predictions of Chaotic Synthesizer Files}{5}\protected@file@percent }
\citation{Bishop}
\citation{Mitchell}
\citation{Geron2}
\citation{Goodfellow}
\citation{Levine}
\citation{Geron}
\citation{Geron}
\citation{Levine}
\citation{Bishop}
\citation{Geron}
\citation{Virtanen}
\citation{White}
\citation{Olson}
\citation{Olson}
\citation{Goodfellow}
\citation{James}
\citation{Virtanen}
\citation{Goodfellow}
\citation{Loy}
\citation{Virtanen}
\@writefile{toc}{\contentsline {section}{\numberline {2}The Neural Network}{6}\protected@file@percent }
\newlabel{sec-TheNeuralNetwork}{{2}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}An Introduction to Neural Networks}{6}\protected@file@percent }
\newlabel{subsec-NerualNetworkIntro}{{2.1}{6}}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\citation{Geron}
\citation{Loy}
\citation{Geron}
\citation{James}
\citation{Loy}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Structure of a Neural Network}{7}\protected@file@percent }
\newlabel{subsec-NetworkStructure}{{2.2}{7}}
\newlabel{eqn-FunctionGraph}{{2}{7}}
\newlabel{eqn-FunctionChain}{{3}{7}}
\newlabel{eqn-altLayerFunction}{{4}{7}}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward propagation system in a standard deep neural network. Each layer is presumed to be a node in a linked computational graph. This example has been setup to assume one input layer, and one output layer. Practical implementations should include mini-batches of data as opposed to a single sample.\relax }}{8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg-FeedForward}{{1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Layers Used in this Classification Model}{8}\protected@file@percent }
\newlabel{subsec-Layers}{{2.3}{8}}
\newlabel{eqn-LinearTransform}{{5}{8}}
\newlabel{eqn-LinearTransform2}{{6}{8}}
\citation{Geron2}
\citation{Loy}
\citation{McCulloch}
\citation{McCulloch}
\citation{Geron}
\citation{Loy}
\citation{Levine}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Tensorflow}
\newlabel{eqn-elementActivation}{{7}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Dense Layer}{9}\protected@file@percent }
\newlabel{subsubsec-DenseLayer}{{2.3.1}{9}}
\newlabel{layer-DenseNeurons}{{8}{9}}
\newlabel{eqn-FunctionDense}{{9}{9}}
\newlabel{eqn-DenseFeedForward}{{11}{9}}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Typical "Call" method for a dense layer in a neural network that contains $n$ neurons/nodes. This example shows the computation over a single input $x^{(l-1)}$ but a practical implementation should include mini-batches of samples.\relax }}{10}\protected@file@percent }
\newlabel{alg-CallDense}{{2}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}2-Dimensional Convolution Layer}{10}\protected@file@percent }
\newlabel{subsubsec-Conv2DLayer}{{2.3.2}{10}}
\newlabel{eqn-convolution}{{12}{10}}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The result of convolving an input (a) with an filter map (b) is a new set of activations (c). This Image was adapted from Goodfellow, pg. 325 \cite  {Goodfellow}\relax }}{11}\protected@file@percent }
\newlabel{fig-2DConvExample}{{1}{11}}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Loy}
\newlabel{eqn-ConvFeedForward}{{14}{12}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Typical "Call" method for a 2-Dimensional Convolutional layer in a neural network that uses $K$ filters, of $m \times n$ kernels, with an assumed stride size of $1 \times 1$. This example shows the computation over a single input $x^{(l-1)}$ but a practical implementation should include mini-batches of samples.\relax }}{12}\protected@file@percent }
\newlabel{alg-CallConv2D}{{3}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}2-Dimensional Maximum Pooling Layer}{12}\protected@file@percent }
\newlabel{subsubsec-2DPool}{{2.3.3}{12}}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The result of 2D maximum-pooling an input array. This image was adapted from Loy, pg. 126 \cite  {Loy}\relax }}{13}\protected@file@percent }
\newlabel{fig-2DMaxPool}{{2}{13}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Typical "Call" method for a 2-Dimensional Maximum Pooling layer in a neural network. We assume 2D input, but a practical implementation may include the need to loop over a high dimensional structure. For simplicity, we assume a stride size of $1 \times 1$. This example shows the computation over a single input $x^{(l-1)}$ but a practical implementation should include mini-batches of samples.\relax }}{13}\protected@file@percent }
\newlabel{alg-CallPool2D}{{4}{13}}
\citation{Geron}
\citation{Loy}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}1-Dimensional Flattening Layer}{14}\protected@file@percent }
\newlabel{subsubsec-1DFlatten}{{2.3.4}{14}}
\newlabel{eqn-FlattenFunction}{{15}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}1-Dimensional Concatenation Layer}{14}\protected@file@percent }
\newlabel{subsubsec-1DConcat}{{2.3.5}{14}}
\newlabel{eqn-ConcatenationFunction}{{18}{14}}
\citation{Geron}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Loy}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Activation Functions Used in Network Layers}{15}\protected@file@percent }
\newlabel{sec-ActivationFunctions}{{2.4}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Rectified Linear Unit}{15}\protected@file@percent }
\newlabel{eqn-ReLU}{{19}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Rectifed Linear Unit (ReLU) activation function\relax }}{15}\protected@file@percent }
\newlabel{fig-ReLU}{{3}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Softmax}{15}\protected@file@percent }
\newlabel{subsubsec-Softmax}{{2.4.2}{15}}
\newlabel{eqn-Softmax}{{20}{15}}
\citation{Goodfellow}
\citation{Mitchell}
\citation{Geron}
\citation{Goodfellow}
\citation{Levine}
\citation{Goodfellow}
\citation{James}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Training The Model}{16}\protected@file@percent }
\newlabel{subsec-Training}{{2.5}{16}}
\newlabel{eqn-Theta}{{21}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}The Cost Function}{16}\protected@file@percent }
\citation{James}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Bishop}
\citation{Goodfellow}
\citation{Loy}
\citation{Mitchell}
\citation{James}
\citation{Loy}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{eqn-CXELoss}{{22}{17}}
\newlabel{eqn-CXELossAvg}{{23}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Gradient Based Learning}{17}\protected@file@percent }
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plot of how $y^*_{j}$ affects the output values of the CXE cost function\relax }}{18}\protected@file@percent }
\newlabel{fig-CXELoss}{{4}{18}}
\newlabel{eqn-CostGradient}{{25}{18}}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Back-Propagation}{19}\protected@file@percent }
\newlabel{subsubsec-BackProp}{{2.5.3}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A visual representation of a simplified neural network as a computational graph\relax }}{20}\protected@file@percent }
\newlabel{fig-ComputationalGraph}{{5}{20}}
\newlabel{eqn-dWL1}{{27}{21}}
\newlabel{eqn-dbL1}{{28}{21}}
\newlabel{eqn-CXELossDeriv}{{29}{21}}
\citation{Geron2}
\citation{Goodfellow}
\citation{Geron}
\newlabel{eqn-dWGeneral}{{34}{22}}
\newlabel{eqn-dbGeneral}{{35}{22}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Backwards propagation system, in a standard densely connected deep neural network. Each iteration in the \textit  {for-loop} computes the gradient of the cost function $J$ with respect to the weight and bias arrays in a given layer $(l)$. Each element in those arrays $dW$ and $db$ is the discrete gradient of the cost due to that parameter. A practical application of this algorithm should include batches of samples instead of a single sample and a regularizing function at each step.\relax }}{22}\protected@file@percent }
\newlabel{alg-BackProp}{{5}{22}}
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Geron}
\newlabel{eqn-GradientLearning}{{36}{23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.4}The Optimizer}{23}\protected@file@percent }
\newlabel{eqn-ADAMupdate}{{37}{23}}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Geron}
\citation{Goodfellow}
\citation{Ngiam}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Adaptive-Moments (ADAM) optimizer for a neural network. This algorithm is adapted from Goodfellow, \cite  {Goodfellow}\relax }}{24}\protected@file@percent }
\newlabel{alg-ADAM}{{6}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Multimodal Architecture}{24}\protected@file@percent }
\newlabel{subsec-Architecture}{{2.6}{24}}
\citation{Li}
\citation{Ngiam}
\citation{White}
\citation{Olson}
\citation{Khan}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The implemented miltimodal architecture of the audio file classification neural network. The Left branch process an image-like input, the right branch processes a vector-like input. The activations are then merged, and then a single output vector is produced\relax }}{26}\protected@file@percent }
\newlabel{fig-NetworkArchitecture}{{6}{26}}
\citation{Goodfellow}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Khan}
\citation{Serizel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}The Spectrogram Branch}{27}\protected@file@percent }
\newlabel{eqn-shapeX1}{{38}{27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}The Perceptron Branch}{27}\protected@file@percent }
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{James}
\newlabel{eqn-shapeX2}{{39}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}The Final Output Branch}{28}\protected@file@percent }
\citation{Virtanen}
\citation{Liu}
\citation{James}
\citation{Serizel}
\citation{Geron}
\@writefile{toc}{\contentsline {section}{\numberline {3}Properties of Musical Instruments}{29}\protected@file@percent }
\newlabel{sec-Instruments}{{3}{29}}
\citation{Hornbostel}
\citation{Hornbostel}
\citation{Olson}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Instruments categories used in the classification task. Note that the network uses only integers given by "Class index" to identify sources. The string "Class Name" is kept for human readability\relax }}{30}\protected@file@percent }
\newlabel{fig-ClassList}{{7}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Idiophones}{30}\protected@file@percent }
\newlabel{subsec-Idiophone}{{3.1}{30}}
\citation{White}
\citation{Hunter}
\citation{Olsen}
\citation{White}
\citation{Hornbostel}
\citation{Olson}
\citation{White}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Spectrogram representation of idiophone waveforms\relax }}{31}\protected@file@percent }
\newlabel{fig-PropertiesIdiophones}{{8}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Membranophones}{31}\protected@file@percent }
\newlabel{subsec-membranophones}{{3.2}{31}}
\citation{White}
\citation{White}
\citation{Olson}
\citation{Hornbostel}
\citation{Haberman}
\citation{Hunter}
\citation{Taylor}
\citation{Olson}
\citation{White}
\citation{Haberman}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Chordophones}{32}\protected@file@percent }
\newlabel{subsec-chordophones}{{3.3}{32}}
\newlabel{eqn-1DWaveEqn}{{40}{32}}
\citation{Hunter}
\citation{Taylor}
\citation{White}
\citation{Haberman}
\citation{Hunter}
\citation{Olson}
\citation{Taylor}
\citation{White}
\citation{Loy}
\citation{Virtanen}
\citation{Serizel}
\citation{Hunter}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Spectrograms for a few Stringed instruments\relax }}{33}\protected@file@percent }
\newlabel{fig-PropertiesChordophones}{{9}{33}}
\citation{Hornbostel}
\citation{Olson}
\citation{White}
\citation{White}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Aerophones}{34}\protected@file@percent }
\newlabel{subsec-Aerophones}{{3.4}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Other Generated Sounds}{34}\protected@file@percent }
\newlabel{subsec-Generated}{{3.5}{34}}
\citation{James}
\citation{Loy}
\citation{Serizel}
\citation{Geron}
\citation{James}
\citation{Khan}
\citation{Mierswa}
\citation{Serizel}
\citation{Liu}
\citation{Olson}
\citation{Virtanen}
\citation{Liu}
\citation{Goodfellow}
\citation{James}
\citation{Serizel}
\citation{Goodfellow}
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\@writefile{toc}{\contentsline {section}{\numberline {4}Feature Selections}{35}\protected@file@percent }
\newlabel{sec-Features}{{4}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Feature Space}{35}\protected@file@percent }
\citation{Virtanen}
\citation{Goodfellow}
\citation{James}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{James}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Decision Boundaries}{36}\protected@file@percent }
\newlabel{subsubsec-Decision}{{4.1.1}{36}}
\citation{James}
\citation{Loy}
\citation{Geron}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Feature Space with Linear separability between two classes\relax }}{37}\protected@file@percent }
\newlabel{fig-LinSep1}{{10}{37}}
\citation{Bishop}
\citation{James}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Feature Space with near linear separability between two classes\relax }}{38}\protected@file@percent }
\newlabel{fig-LinSep2}{{11}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Feature Space with near linear non-separability between two classes\relax }}{38}\protected@file@percent }
\newlabel{fig-LinNonSep}{{12}{38}}
\citation{Goodfellow}
\citation{James}
\citation{Bishop}
\citation{Geron}
\citation{Li}
\citation{Ngiam}
\citation{Tensorflow}
\citation{Loy}
\citation{Geron2}
\citation{James}
\citation{Serizel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}The Design Matrix}{39}\protected@file@percent }
\newlabel{subsubsec-DesignMatrix}{{4.1.2}{39}}
\newlabel{eqn-X1Shape}{{43}{39}}
\newlabel{eqn-X2Shape}{{44}{39}}
\newlabel{eqn-YShape}{{45}{39}}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Audio Preprocessing}{40}\protected@file@percent }
\newlabel{subsubsec-Preprocessing}{{4.1.3}{40}}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\citation{Mierswa}
\citation{Liu}
\citation{Zhang}
\citation{Khan}
\citation{Serizel}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Spectrogram Features}{41}\protected@file@percent }
\newlabel{subsec-Spectrogram}{{4.2}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Spectrogram representations of various waveforms\relax }}{41}\protected@file@percent }
\newlabel{fig-Spectrograms}{{13}{41}}
\citation{Liu}
\citation{Liu}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Frame Blocking}{42}\protected@file@percent }
\newlabel{subsubsec-FrameBlocking}{{4.2.1}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces A visualization of how frame-blocking is used to create each analysis frames. This image has been adapted and modified from Liu, et. al. "Audio Feature Extraction and Analysis", Fig. (1). See ref. \cite  {Liu}.\relax }}{42}\protected@file@percent }
\newlabel{fig-AnalysisFrames}{{14}{42}}
\newlabel{eqn-FrameMatrix}{{46}{42}}
\newlabel{eqn-IndexingA}{{47}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Windowing}{43}\protected@file@percent }
\newlabel{subsubsec-Windowing}{{4.2.2}{43}}
\newlabel{eqn-Hanning}{{48}{43}}
\newlabel{eqn-WindowMatrix}{{49}{43}}
\citation{Virtanen}
\citation{Olson}
\citation{Peatross}
\citation{Virtanen}
\citation{Taylor}
\citation{Virtanen}
\citation{Short}
\citation{Peatross}
\citation{Levine}
\citation{Loy}
\citation{Tensorflow}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Discrete Fourier Transform}{44}\protected@file@percent }
\newlabel{subsubsec-DFT}{{4.2.3}{44}}
\newlabel{eqn-DFTMatrix}{{50}{44}}
\newlabel{eqn-DFT}{{51}{44}}
\newlabel{eqn-Spectrogram}{{52}{44}}
\citation{Olson}
\citation{Virtanen}
\citation{White}
\newlabel{eqn-IndexingS}{{53}{45}}
\newlabel{eqn-X1}{{54}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Time-Space Features}{45}\protected@file@percent }
\newlabel{subsec-TimeFeatures}{{4.3}{45}}
\citation{Serizel}
\citation{Liu}
\citation{Olson}
\citation{Serizel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Time Domain Envelope}{46}\protected@file@percent }
\newlabel{eqn-RMS}{{55}{46}}
\citation{Virtanen}
\citation{Olson}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces TDE Envelope values for musical instruments\relax }}{47}\protected@file@percent }
\newlabel{fig-ExampleTDE}{{16}{47}}
\citation{Khan}
\citation{Liu}
\citation{Zhang}
\citation{Serizel}
\citation{Liu}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces A comparison of the first $3$ of $5$ Time Domain Envelope across each class using a box-and-whisker plot\relax }}{48}\protected@file@percent }
\newlabel{fig-FeatureTDE}{{17}{48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Zero Crossing Rate}{48}\protected@file@percent }
\citation{Liu}
\citation{White}
\newlabel{eqn-ZXR}{{56}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces A comparison of the Zero-Crossing Rate for each class using a box-and-whisker plot\relax }}{49}\protected@file@percent }
\newlabel{fig-FeatureZXR}{{18}{49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Temporal Center of Mass}{49}\protected@file@percent }
\newlabel{eqn-FeatureTCM}{{57}{49}}
\citation{Olson}
\citation{White}
\citation{Serizel}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces TCM values for musical instruments\relax }}{50}\protected@file@percent }
\newlabel{fig-ExampleTCM}{{19}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces A comparison of the temporal center mass for each class using a box-and-whisker plot\relax }}{50}\protected@file@percent }
\newlabel{fig-FeatureTCM}{{20}{50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Auto Correlation Coefficients}{50}\protected@file@percent }
\citation{Serizel}
\newlabel{eqn-FeatureACC}{{58}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces A comparison of the the first four auto correlation coefficients in each class using a box-and-whisker plot\relax }}{52}\protected@file@percent }
\newlabel{fig-FeatureACC}{{21}{52}}
\citation{Sahidullah}
\citation{Serizel}
\citation{Sahidullah}
\citation{Serizel}
\citation{Serizel}
\citation{Khan}
\citation{Olson}
\citation{Serizel}
\citation{Khan}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Frequency-Space Features}{53}\protected@file@percent }
\newlabel{subsec-FreqFeatures}{{4.4}{53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Mel Filter Bank Energies}{53}\protected@file@percent }
\newlabel{eqn-HztoMel}{{59}{53}}
\newlabel{eqn-MeltoHz}{{60}{53}}
\citation{Serizel}
\citation{Sahidullah}
\citation{Virtanen}
\citation{Serizel}
\citation{Sahidullah}
\citation{Liu}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Mel Filter Banks shown in frequency space with units of Hertz\relax }}{54}\protected@file@percent }
\newlabel{fig-MelFilterBanks}{{22}{54}}
\newlabel{eqn-FilterBanks}{{61}{54}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Mel Frequency Cepstral Coeffecients}{54}\protected@file@percent }
\newlabel{feat-MFCC}{{62}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces A comparison of 4 Mel Frequency Cepstral Coefficients, $1$, $4$, $8$ and $12$ for each class using box-and-whisker plots\relax }}{56}\protected@file@percent }
\newlabel{fig-FeatureMFCCs}{{23}{56}}
\citation{Olson}
\citation{White}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Frequency Center of Mass}{57}\protected@file@percent }
\newlabel{eqn-FeatureFCM}{{63}{57}}
\newlabel{fig-FeatureFCM}{{\caption@xref {fig-FeatureFCM}{ on input line 1589}}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces A comparison of the frequency center-of-mass for each class using box-and-whisker plots\relax }}{57}\protected@file@percent }
\citation{Geron}
\citation{Goodfellow}
\citation{Geron2}
\citation{Mitchell}
\citation{Geron}
\citation{Geron}
\citation{James}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluating Model Performance}{58}\protected@file@percent }
\newlabel{sec-PerfEval}{{5}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}K-Folds Cross Validation}{58}\protected@file@percent }
\newlabel{subsec-XValidation}{{5.1}{58}}
\newlabel{eqn-XValSplit}{{65}{58}}
\citation{James}
\citation{Geron}
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces A $K$-Fold Cross Validation program.\relax }}{59}\protected@file@percent }
\newlabel{alg-CrossValidation}{{7}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Performance Metrics}{59}\protected@file@percent }
\citation{Geron}
\citation{Geron}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Confusion Matrix}{60}\protected@file@percent }
\newlabel{eqn-ConfMat}{{5.2.1}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Example Confusion Matrices for $4$-categories classifier\relax }}{60}\protected@file@percent }
\newlabel{fig-DummyConfMat}{{25}{60}}
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{James}
\citation{Geron}
\citation{James}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Accuracy Score}{61}\protected@file@percent }
\newlabel{eqn-BinaryAccuracy}{{66}{61}}
\citation{Geron}
\citation{James}
\newlabel{eqn-Accuracy}{{67}{62}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Precision Score}{62}\protected@file@percent }
\newlabel{eqn-BinaryPrecision}{{69}{62}}
\newlabel{eqn-KPrecision}{{70}{62}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Recall Score}{62}\protected@file@percent }
\newlabel{eqn-BinaryRecall}{{71}{62}}
\citation{Geron}
\citation{James}
\citation{Geron}
\citation{Geron}
\citation{James}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{James}
\citation{Loy}
\newlabel{eqn-KRecall}{{72}{63}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.5}F1-Score}{63}\protected@file@percent }
\newlabel{eqn-F1Score}{{73}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Tracking Metrics over a Period of Training}{63}\protected@file@percent }
\newlabel{subsec-TrainingMetrics}{{5.3}{63}}
\@writefile{toc}{\contentsline {paragraph}{}{63}\protected@file@percent }
\citation{Geron}
\citation{James}
\citation{Geron}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces The loss function score decreases with each training step, indicating that optimization is performing correctly\relax }}{65}\protected@file@percent }
\newlabel{fig-LossScore}{{26}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces The precision score and recall score generally increase with each training step\relax }}{65}\protected@file@percent }
\newlabel{fig-PrecisionRecallScores}{{27}{65}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experimental Results}{66}\protected@file@percent }
\newlabel{sec-Results}{{6}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Executing Cross Validation}{66}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Performance metrics for the multimodal networks across $10$ models, scores are averaged over $37$ classes\relax }}{66}\protected@file@percent }
\newlabel{fig-MultimodalXval}{{28}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Confusion matrices, each is averaged over 10 folds of cross validation\relax }}{67}\protected@file@percent }
\newlabel{fig-MultimodalConfs}{{29}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Comparing Results between Architectures}{67}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces \relax }}{68}\protected@file@percent }
\newlabel{fig-UnimodalXVal}{{30}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Performance metrics for the unimodal networks across $10$ models, scores are averaged over $37$ classes\relax }}{69}\protected@file@percent }
\newlabel{fig-UnimodalConfs}{{31}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Comparing Classification Scores within Each Class}{69}\protected@file@percent }
\newlabel{subsec-ClassScores}{{6.3}{69}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}High Woodwind Scores}{70}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces \relax }}{70}\protected@file@percent }
\newlabel{fig-HighWindsScores}{{32}{70}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Middle and Low Woodwind Scores}{71}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces \relax }}{71}\protected@file@percent }
\newlabel{fig-LowWindsScores}{{33}{71}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3}Brass Scores}{72}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces \relax }}{72}\protected@file@percent }
\newlabel{fig-BrassScores}{{34}{72}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.4}String Scores}{73}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces \relax }}{73}\protected@file@percent }
\newlabel{fig-StringScores}{{35}{73}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.5}Percussion Scores}{74}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces \relax }}{74}\protected@file@percent }
\newlabel{fig-PercussionScores}{{36}{74}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.6}Synthetic Waveform Scores}{75}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces \relax }}{75}\protected@file@percent }
\newlabel{fig-SynthScores}{{37}{75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Discussion of Results}{75}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Predictions on Chaotic Synthesizers}{76}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{77}\protected@file@percent }
\newlabel{sec-Conclusion}{{7}{77}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgments}{78}\protected@file@percent }
\newlabel{sec-Acknowledge}{{8}{78}}
\bibstyle{apalike}
\bibcite{Bishop}{1}
\bibcite{Geron}{2}
\bibcite{Geron2}{3}
\bibcite{Goodfellow}{4}
\bibcite{Haberman}{5}
\bibcite{Hornbostel}{6}
\bibcite{Hunter}{7}
\bibcite{James}{8}
\bibcite{Khan}{9}
\bibcite{Levine}{10}
\bibcite{Li}{11}
\bibcite{Liu}{12}
\bibcite{Loy}{13}
\bibcite{McCulloch}{14}
\bibcite{Mierswa}{15}
\bibcite{Mitchell}{16}
\bibcite{Ngiam}{17}
\bibcite{Olson}{18}
\bibcite{Peatross}{19}
\bibcite{Petrik}{20}
\bibcite{Powers}{21}
\bibcite{Sahidullah}{22}
\bibcite{Serizel}{23}
\bibcite{Short}{24}
\bibcite{Taylor}{25}
\bibcite{Tensorflow}{26}
\bibcite{Virtanen}{27}
\bibcite{White}{28}
\bibcite{Zhang}{29}
