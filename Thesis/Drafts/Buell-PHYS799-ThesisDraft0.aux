\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}\protected@file@percent }
\newlabel{sec-Introduction}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{3}\protected@file@percent }
\newlabel{sec-Methodology}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Designing the Function}{3}\protected@file@percent }
\newlabel{eqn-MappingFunction}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Collecting and Pre-processing Raw Data}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Designing Classification Features}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Designing A Complementary Network Architecture}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Testing and Evaluating Network Performance}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Running Predictions of Chaotic Synthesizer Files}{4}\protected@file@percent }
\citation{Geron2}
\citation{Goodfellow}
\citation{Levine}
\citation{Goodfellow}
\citation{James}
\citation{Virtanen}
\citation{Geron}
\citation{Olsen}
\citation{White}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Loy}
\@writefile{toc}{\contentsline {section}{\numberline {3}The Neural Network}{5}\protected@file@percent }
\newlabel{sec-TheNeuralNetwork}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}An Introduction to Neural Networks}{5}\protected@file@percent }
\newlabel{subsec-NerualNetworkIntro}{{3.1}{5}}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{James}
\citation{Loy}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Structure of a Neural Network}{6}\protected@file@percent }
\newlabel{subsec-NetworkStructure}{{3.2}{6}}
\newlabel{eqn-FunctionChain}{{2}{6}}
\newlabel{eqn-LayerFunction}{{3}{6}}
\newlabel{eqn-altLayerFunction}{{4}{6}}
\citation{Goodfellow}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward propagation system in a standard deep neural network. Each iteration in the main \textit  {for-loop} represents the execution of a layer, and passing the result to the "next" layer function. We assume each layter to follow a two-step structure, being (i) a linear transformation as in eqn. (5\hbox {}) and (ii) an element-wise non-linear activation function as in eqn. (7\hbox {}). A practical application of this algorithm should include batches of samples instead of a single sample. \leavevmode {\color  {red}I Want to include this algorithm for context - especially on the programming end, but am not sure where to put it. }\relax }}{7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algFeedForward}{{1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Layers Used in Classification Neural Network}{7}\protected@file@percent }
\newlabel{subsec-Layers}{{3.3}{7}}
\newlabel{eqn-LinearTransform}{{5}{7}}
\citation{Geron2}
\citation{Loy}
\citation{McCulloch}
\citation{Geron}
\citation{Geron}
\citation{Loy}
\newlabel{eqn-LinearTransform2}{{6}{8}}
\newlabel{eqn-elementActivation}{{7}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Dense Layer}{8}\protected@file@percent }
\newlabel{subsubsec-DenseLayer}{{3.3.1}{8}}
\newlabel{layer-DenseNeurons}{{8}{8}}
\newlabel{eqn-FunctionDense}{{9}{8}}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{eqn-DenseFeedForward}{{11}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}2-Dimensional Convolution Layer}{9}\protected@file@percent }
\newlabel{subsubsec-Conv2DLayer}{{3.3.2}{9}}
\newlabel{eqn-convolution}{{12}{9}}
\citation{Goodfellow}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Loy}
\newlabel{fig-2DConvExample}{{\caption@xref {fig-2DConvExample}{ on input line 316}}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The result of convolving an input (a) with an filter map (b) is a new set of activations (c). This Image was adapted from Goodfellow, pg. 325 \cite  {Goodfellow}\relax }}{10}\protected@file@percent }
\newlabel{eqn-ConvFeedForward}{{14}{10}}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}2-Dimensional Maximum Pooling Layer}{11}\protected@file@percent }
\newlabel{fig-2DMaxPool}{{\caption@xref {fig-2DMaxPool}{ on input line 352}}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The result of 2D maximum-pooling an input array. This image was adapted from Loy, pg. 126 \cite  {Loy}\relax }}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}1-Dimensional Flattening Layer}{11}\protected@file@percent }
\newlabel{eqn-FlattenFunction}{{15}{11}}
\citation{Geron}
\citation{Loy}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.5}1-Dimensional Concatenation Layer}{12}\protected@file@percent }
\newlabel{eqn-ConcatenationFunction}{{18}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Activation Functions Used in Network Layers}{12}\protected@file@percent }
\newlabel{sec-ActivationFunctions}{{3.4}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Rectified Linear Unit}{12}\protected@file@percent }
\newlabel{eqn-ReLU}{{19}{12}}
\citation{Geron}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Goodfellow}
\citation{Mitchell}
\newlabel{fig-ReLU}{{\caption@xref {fig-ReLU}{ on input line 419}}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Rectifed Linear Unit (ReLU) activation function\relax }}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Softmax}{13}\protected@file@percent }
\newlabel{eqn-Softmax}{{20}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Training a Neural Network}{13}\protected@file@percent }
\newlabel{subsec-Training}{{3.5}{13}}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{James}
\citation{James}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{eqn-Theta}{{21}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}The Cost Function}{14}\protected@file@percent }
\newlabel{eqn-CategoricalCrossentropy}{{22}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Gradient Based Learning}{14}\protected@file@percent }
\citation{Goodfellow}
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\citation{Geron}
\newlabel{eqn-CostGradient}{{23}{15}}
\citation{Geron}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backwards propagation system, in a standard densely connected deep neural network. Each iteration in the \textit  {for-loop} computes the gradient of the cost function $J$ with respect to the weight and bias arrays. Each element in those arrays is then the discrete gradient of that parameter. A practical application of this algorithm should include batches of samples instead of a single sample\relax }}{16}\protected@file@percent }
\newlabel{algBackProp}{{2}{16}}
\newlabel{eqn-GradientLearning}{{24}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}The Optimizer}{16}\protected@file@percent }
\citation{Geron}
\newlabel{eqn-ADAMupdate}{{25}{17}}
\citation{Geron}
\citation{Kahn}
\citation{Liu}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Adaptive-Moments (ADAM) optimizer for a neural network\relax }}{18}\protected@file@percent }
\newlabel{algAdaGrad}{{3}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Chosen Model Architecture}{18}\protected@file@percent }
\newlabel{sec-Architecture}{{3.6}{18}}
\citation{White}
\citation{Olsen}
\citation{Kahn}
\newlabel{fig-NetworkArchitecture}{{\caption@xref {fig-NetworkArchitecture}{ on input line 602}}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The developed architecture of the audio file classification neural network. The Left branch process an image-like input, the right branch processes a vector-like input. The activations are then merged, and then a single output is produced\relax }}{20}\protected@file@percent }
\citation{Geron}
\citation{Kahn}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1}The Spectrogram Branch}{21}\protected@file@percent }
\newlabel{eqn-shapeX1}{{26}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2}The Perceptron Branch}{21}\protected@file@percent }
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{James}
\newlabel{eqn-shapeX2}{{27}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.3}The Final Output Branch}{22}\protected@file@percent }
\citation{Khan}
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\citation{Virtanen}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Liu}
\@writefile{toc}{\contentsline {section}{\numberline {4}Feature Selections}{23}\protected@file@percent }
\newlabel{sec-Features}{{4}{23}}
\citation{Geron2}
\citation{James}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.0.1}The Design Matrix}{24}\protected@file@percent }
\newlabel{eqn-X1Shape}{{28}{24}}
\newlabel{eqn-X2Shape}{{29}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.0.2}Audio Preprocessing}{24}\protected@file@percent }
\newlabel{subsubsec-Preprocessing}{{4.0.2}{24}}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\citation{Mierswa}
\citation{Liu}
\citation{Zhang}
\citation{Kahn}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Spectrogram Feature}{25}\protected@file@percent }
\newlabel{subsec-spectrogram}{{4.1}{25}}
\newlabel{fig-spectrograms}{{\caption@xref {fig-spectrograms}{ on input line 727}}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Spectrogram representations of various waveforms\relax }}{25}\protected@file@percent }
\newlabel{eqn-FrameMatrix}{{30}{25}}
\newlabel{eqn-IndexingA}{{31}{25}}
\citation{Virtanen}
\citation{Olson}
\citation{Peatross}
\citation{Short}
\citation{Peatross}
\newlabel{eqn-Hann}{{32}{26}}
\newlabel{eqn-WindowMatrix}{{33}{26}}
\newlabel{eqn-DFTMatrix}{{34}{26}}
\newlabel{eqn-DFT}{{35}{26}}
\newlabel{eqn-Spectrogram}{{36}{26}}
\newlabel{eqn-IndexingS}{{37}{26}}
\citation{Olson}
\citation{Virtanen}
\citation{White}
\newlabel{eqn-X1 shape}{{38}{27}}
\citation{Olson}
\citation{Virtanen}
\citation{Liu}
\citation{Kahn}
\citation{Zhang}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Time-Space Features}{28}\protected@file@percent }
\newlabel{subsec-time}{{4.2}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Time Domain Envelope}{28}\protected@file@percent }
\newlabel{eqn-RMS}{{39}{28}}
\newlabel{fig-FeatureTDE}{{\caption@xref {fig-FeatureTDE}{ on input line 846}}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Time domain envelope visualized in feature-space\relax }}{28}\protected@file@percent }
\citation{Virtanen}
\citation{Liu}
\citation{Virtanen}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Zero Crossing Rate}{29}\protected@file@percent }
\newlabel{eqn-ZXR}{{40}{29}}
\newlabel{fig-FeatureZXR}{{\caption@xref {fig-FeatureZXR}{ on input line 867}}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Zero crossing rate visualized in feature-space\relax }}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Center of Mass}{29}\protected@file@percent }
\newlabel{eqn-FeatureTCM}{{41}{29}}
\newlabel{fig-FeatureTCM}{{\caption@xref {fig-FeatureTCM}{ on input line 888}}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Temporal center-of-mass visualized in feature-space\relax }}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Auto Correlation Coefficients}{30}\protected@file@percent }
\newlabel{eqn-FeatureACC}{{42}{30}}
\newlabel{fig-FeatureACC}{{\caption@xref {fig-FeatureACC}{ on input line 907}}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces First four auto correlation coefficients visualized in feature-space\relax }}{30}\protected@file@percent }
\citation{Liu}
\citation{Virtanen}
\citation{Zhang}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Frequency-Space Features}{31}\protected@file@percent }
\newlabel{subsec-frequency}{{4.3}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Mel Frequency Cepstral Coefficients}{31}\protected@file@percent }
\newlabel{eqn-HztoMel}{{43}{31}}
\newlabel{eqn-MeltoHz}{{44}{31}}
\newlabel{fig-MelFilterBanks}{{\caption@xref {fig-MelFilterBanks}{ on input line 947}}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Mel Filter Banks shown in frequency space with units of Hertz\relax }}{31}\protected@file@percent }
\newlabel{eqn-FilterBanks}{{45}{31}}
\citation{Olson}
\citation{White}
\newlabel{fig-FeatureMFCC}{{\caption@xref {fig-FeatureMFCC}{ on input line 963}}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Mel Frequency Ceptral Coefficients in feature-space\relax }}{32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Center of Mass}{32}\protected@file@percent }
\newlabel{eqn-FeatureFCM}{{46}{32}}
\newlabel{fig-FeatureFCM}{{\caption@xref {fig-FeatureFCM}{ on input line 988}}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Frequency center-of-mass visualized in feature-space\relax }}{32}\protected@file@percent }
\citation{Goodfellow}
\citation{Geron2}
\citation{Mitchell}
\citation{James}
\citation{Geron}
\citation{James}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluating Performance}{33}\protected@file@percent }
\newlabel{sec-PerfEval}{{5}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Cross Validation}{33}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces A $K$-Fold Cross Validation program.\relax }}{33}\protected@file@percent }
\newlabel{algFeedForward}{{4}{33}}
\citation{Geron}
\citation{Geron}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Performance Metrics}{34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Confusion Matrix}{34}\protected@file@percent }
\newlabel{eqn-ConfMat}{{5.2.1}{34}}
\newlabel{fig-ConfMat}{{\caption@xref {fig-ConfMat}{ on input line 1068}}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Example Confusion Matrices for $k$-classes tasks\relax }}{34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Precision Score}{35}\protected@file@percent }
\newlabel{eqn-BinaryPrecision}{{48}{35}}
\newlabel{eqn-KPrecision}{{49}{35}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Recall Score}{35}\protected@file@percent }
\newlabel{eqn-BinaryRecall}{{50}{35}}
\newlabel{eqn-KRecall}{{51}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Tracking Metrics over a Period of Training}{35}\protected@file@percent }
\newlabel{subsec-TrainingMetrics}{{5.3}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experimental Results}{36}\protected@file@percent }
\newlabel{sec-Results}{{6}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{37}\protected@file@percent }
\newlabel{sec-Conclusion}{{7}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgments}{38}\protected@file@percent }
\newlabel{sec-Acknowledge}{{8}{38}}
\bibstyle{apalike}
\bibcite{Geron}{1}
\bibcite{Geron2}{2}
\bibcite{Goodfellow}{3}
\bibcite{James}{4}
\bibcite{Kahn}{5}
\bibcite{Levine}{6}
\bibcite{Liu}{7}
\bibcite{Loy}{8}
\bibcite{McCulloch}{9}
\bibcite{Mierswa}{10}
\bibcite{Mitchell}{11}
\bibcite{Olsen}{12}
\bibcite{Peatross}{13}
\bibcite{Petrik}{14}
\bibcite{Short}{15}
\bibcite{Tenorflow}{16}
\bibcite{Virtanen}{17}
\bibcite{White}{18}
\bibcite{Zhang}{19}
