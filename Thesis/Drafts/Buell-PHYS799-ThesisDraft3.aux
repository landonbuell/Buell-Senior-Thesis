\relax 
\citation{Khan}
\citation{Mierswa}
\citation{Virtanen}
\citation{Bishop}
\citation{Geron}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Levine}
\citation{Loy}
\citation{Khan}
\citation{Liu}
\citation{Mierswa}
\citation{Serizel}
\citation{Zhang}
\citation{Virtanen}
\citation{Liu}
\citation{Khan}
\citation{Mierswa}
\citation{Serizel}
\citation{Liu}
\citation{Zhang}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}\protected@file@percent }
\newlabel{sec-Introduction}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Introduction}{3}\protected@file@percent }
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{McCulloch}
\citation{Loy}
\citation{Geron}
\citation{Petrik}
\citation{Li}
\citation{Ngiam}
\citation{Loy}
\citation{Goodfellow}
\citation{Philharmonia}
\citation{UnivIowa}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Methodology}{5}\protected@file@percent }
\newlabel{sec-Methodology}{{1.2}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Designing the Function}{5}\protected@file@percent }
\newlabel{eqn-MappingFunction}{{1}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Collecting and Pre-processing Raw Data}{5}\protected@file@percent }
\citation{Goodfellow}
\citation{Tensorflow}
\citation{numpy}
\citation{scipy}
\citation{sklearn}
\citation{matplotlib}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Designing Classification Features}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.4}Designing A Complementary Network Architecture}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.5}Testing and Evaluating Network Performance}{6}\protected@file@percent }
\citation{Bishop}
\citation{Mitchell}
\citation{Geron2}
\citation{Goodfellow}
\citation{Levine}
\citation{Mitchell}
\citation{Geron}
\citation{Levine}
\citation{Levine}
\citation{Bishop}
\citation{Geron}
\citation{Levine}
\citation{Goodfellow}
\citation{Geron}
\citation{Geron}
\citation{Virtanen}
\citation{White}
\citation{Olson}
\citation{Olson}
\citation{Goodfellow}
\citation{James}
\citation{Virtanen}
\citation{Geron}
\citation{Levine}
\citation{Goodfellow}
\citation{Loy}
\citation{Virtanen}
\citation{Tensorflow}
\@writefile{toc}{\contentsline {section}{\numberline {2}The Neural Network}{7}\protected@file@percent }
\newlabel{sec-TheNeuralNetwork}{{2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}An Introduction to Neural Networks}{7}\protected@file@percent }
\newlabel{subsec-NerualNetworkIntro}{{2.1}{7}}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\citation{Geron}
\citation{Loy}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Structure of a Neural Network}{8}\protected@file@percent }
\newlabel{subsec-NetworkStructure}{{2.2}{8}}
\newlabel{eqn-FunctionGraph}{{2}{8}}
\newlabel{eqn-FunctionChain}{{3}{8}}
\citation{Geron}
\citation{James}
\citation{Loy}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{eqn-altLayerFunction}{{4}{9}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward propagation system in a standard deep neural network. Each layer is presumed to be a node in a linked computational graph. This example has been setup to assume one input layer, and one output layer. Practical implementations should include mini-batches of data as opposed to a single sample.\relax }}{9}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg-FeedForward}{{1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Layers Used in this Classification Model}{9}\protected@file@percent }
\newlabel{subsec-Layers}{{2.3}{9}}
\citation{Geron2}
\citation{Loy}
\citation{McCulloch}
\citation{McCulloch}
\citation{Geron}
\citation{Loy}
\citation{Levine}
\newlabel{eqn-LinearTransform}{{5}{10}}
\newlabel{eqn-LinearTransform2}{{6}{10}}
\newlabel{eqn-elementActivation}{{7}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Dense Layer}{10}\protected@file@percent }
\newlabel{subsubsec-DenseLayer}{{2.3.1}{10}}
\newlabel{layer-DenseNeurons}{{8}{10}}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Tensorflow}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Goodfellow}
\newlabel{eqn-FunctionDense}{{9}{11}}
\newlabel{eqn-DenseFeedForward}{{11}{11}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Typical "Call" method for a dense layer in a neural network that contains $n$ neurons/nodes. This example shows the computation over a single input $x^{(l-1)}$ but a practical implementation should include mini-batches of samples.\relax }}{11}\protected@file@percent }
\newlabel{alg-CallDense}{{2}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}2-Dimensional Convolution Layer}{11}\protected@file@percent }
\newlabel{subsubsec-Conv2DLayer}{{2.3.2}{11}}
\newlabel{eqn-convolution}{{12}{11}}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The result of convolving an input (a) with an filter map (b) is a new set of activations (c). This Image was adapted from Goodfellow, pg. 325 \cite  {Goodfellow}\relax }}{13}\protected@file@percent }
\newlabel{fig-2DConvExample}{{1}{13}}
\newlabel{eqn-ConvFeedForward}{{14}{13}}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Loy}
\citation{Loy}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Typical "Call" method for a 2-Dimensional Convolutional layer in a neural network that uses $K$ filters, of $m \times n$ kernels, with an assumed stride size of $1 \times 1$. This example shows the computation over a single input $x^{(l-1)}$ but a practical implementation should include mini-batches of samples.\relax }}{14}\protected@file@percent }
\newlabel{alg-CallConv2D}{{3}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}2-Dimensional Maximum Pooling Layer}{14}\protected@file@percent }
\newlabel{subsubsec-2DPool}{{2.3.3}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The result of 2D maximum-pooling an input array. This image was adapted from Loy, pg. 126 \cite  {Loy}\relax }}{14}\protected@file@percent }
\newlabel{fig-2DMaxPool}{{2}{14}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Typical "Call" method for a 2-Dimensional Maximum Pooling layer in a neural network. We assume 2D input, but a practical implementation may include the need to loop over a high dimensional structure. For simplicity, we assume a stride size of $1 \times 1$. This example shows the computation over a single input $x^{(l-1)}$ but a practical implementation should include mini-batches of samples.\relax }}{15}\protected@file@percent }
\newlabel{alg-CallPool2D}{{4}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}1-Dimensional Flattening Layer}{15}\protected@file@percent }
\newlabel{subsubsec-1DFlatten}{{2.3.4}{15}}
\newlabel{eqn-FlattenFunction}{{15}{15}}
\citation{Geron}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}1-Dimensional Concatenation Layer}{16}\protected@file@percent }
\newlabel{subsubsec-1DConcat}{{2.3.5}{16}}
\newlabel{eqn-ConcatenationFunction}{{18}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Activation Functions Used in Network Layers}{16}\protected@file@percent }
\newlabel{sec-ActivationFunctions}{{2.4}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Rectified Linear Unit}{16}\protected@file@percent }
\newlabel{eqn-ReLU}{{19}{16}}
\citation{Geron}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Goodfellow}
\citation{Mitchell}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Rectifed Linear Unit (ReLU) activation function\relax }}{17}\protected@file@percent }
\newlabel{fig-ReLU}{{3}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Softmax}{17}\protected@file@percent }
\newlabel{subsubsec-Softmax}{{2.4.2}{17}}
\newlabel{eqn-Softmax}{{20}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Training The Model}{17}\protected@file@percent }
\newlabel{subsec-Training}{{2.5}{17}}
\citation{Geron}
\citation{Goodfellow}
\citation{Levine}
\citation{Goodfellow}
\citation{James}
\citation{James}
\citation{Bishop}
\citation{Goodfellow}
\citation{Virtanen}
\newlabel{eqn-Theta}{{21}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}The Cost Function}{18}\protected@file@percent }
\citation{Goodfellow}
\citation{Loy}
\citation{Mitchell}
\citation{Goodfellow}
\newlabel{eqn-CXELoss}{{22}{19}}
\newlabel{eqn-CXELossAvg}{{23}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plot of how $y^*_{j} \in [0,1]$ affects the output values of the CXE cost function\relax }}{19}\protected@file@percent }
\newlabel{fig-CXELoss}{{4}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Gradient Based Learning}{19}\protected@file@percent }
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\newlabel{eqn-CostGradient}{{25}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Back-Propagation}{20}\protected@file@percent }
\newlabel{subsubsec-BackProp}{{2.5.3}{20}}
\citation{Geron}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A visual representation of a simplified neural network as a computational graph\relax }}{22}\protected@file@percent }
\newlabel{fig-ComputationalGraph}{{5}{22}}
\newlabel{eqn-dWL1}{{27}{23}}
\newlabel{eqn-dbL1}{{28}{23}}
\newlabel{eqn-CXELossDeriv}{{29}{23}}
\citation{Geron2}
\citation{Goodfellow}
\citation{Geron}
\citation{Geron}
\citation{Goodfellow}
\newlabel{eqn-dWGeneral}{{34}{24}}
\newlabel{eqn-dbGeneral}{{35}{24}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Backwards propagation system, in a standard densely connected deep neural network. Each iteration in the \textit  {for-loop} computes the gradient of the cost function $J$ with respect to the weight and bias arrays in a given layer $(l)$. Each element in those arrays $dW$ and $db$ is the discrete gradient of the cost due to that parameter. A practical application of this algorithm should include batches of samples instead of a single sample and a regularizing function at each step.\relax }}{24}\protected@file@percent }
\newlabel{alg-BackProp}{{5}{24}}
\citation{Goodfellow}
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Geron}
\newlabel{eqn-GradientLearning}{{36}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.4}The Optimizer}{25}\protected@file@percent }
\newlabel{subsubsec-Optimizer}{{2.5.4}{25}}
\newlabel{eqn-ADAMupdate}{{37}{25}}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Geron}
\citation{Goodfellow}
\citation{Ngiam}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Adaptive-Moments (ADAM) optimizer for a neural network. This algorithm is adapted from Goodfellow \cite  {Goodfellow}\relax }}{26}\protected@file@percent }
\newlabel{alg-ADAM}{{6}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Multimodal Architecture}{26}\protected@file@percent }
\newlabel{subsec-Architecture}{{2.6}{26}}
\citation{Li}
\citation{Ngiam}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The implemented multimodal architecture of the audio file classification neural network. The left branch process an image-like input, and the right branch processes a vector-like input. The activations are then merged, and then a single output vector is produced\relax }}{28}\protected@file@percent }
\newlabel{fig-NetworkArchitecture}{{6}{28}}
\citation{White}
\citation{Olson}
\citation{Khan}
\citation{Goodfellow}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Khan}
\citation{Serizel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}The Convolution Branch}{29}\protected@file@percent }
\newlabel{eqn-shapeX1}{{38}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}The Perceptron Branch}{29}\protected@file@percent }
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{James}
\citation{Sklearn}
\newlabel{eqn-shapeX2}{{39}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}The Final Output Branch}{30}\protected@file@percent }
\citation{Virtanen}
\citation{Liu}
\citation{James}
\citation{Serizel}
\citation{Geron}
\citation{Levine}
\@writefile{toc}{\contentsline {section}{\numberline {3}Properties of Musical Instruments}{31}\protected@file@percent }
\newlabel{sec-Instruments}{{3}{31}}
\citation{Hornbostel}
\citation{Hornbostel}
\citation{Olson}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Instruments categories used in the classification task. Note that the network uses only integers given by "Class index" to identify sources. The string "Class Name" is kept for human readability\relax }}{32}\protected@file@percent }
\newlabel{fig-ClassList}{{7}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Idiophones}{32}\protected@file@percent }
\newlabel{subsec-Idiophone}{{3.1}{32}}
\citation{White}
\citation{Hunter}
\citation{Olson}
\citation{White}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Spectrograms from idiophone waveforms\relax }}{33}\protected@file@percent }
\newlabel{fig-PropertiesIdiophones}{{8}{33}}
\citation{Hornbostel}
\citation{Olson}
\citation{White}
\citation{White}
\citation{White}
\citation{Olson}
\citation{Hornbostel}
\citation{Haberman}
\citation{Hunter}
\citation{Taylor}
\citation{Taylor}
\citation{Olson}
\citation{White}
\citation{Haberman}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Membranophones}{34}\protected@file@percent }
\newlabel{subsec-membranophones}{{3.2}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Chordophones}{34}\protected@file@percent }
\newlabel{subsec-chordophones}{{3.3}{34}}
\newlabel{eqn-1DWaveEqn}{{40}{34}}
\citation{Hunter}
\citation{Hunter}
\citation{Taylor}
\citation{White}
\citation{Haberman}
\citation{Hunter}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Spectrograms from chordophone waveforms\relax }}{35}\protected@file@percent }
\newlabel{fig-PropertiesChordophones}{{9}{35}}
\citation{Loy}
\citation{Virtanen}
\citation{Serizel}
\citation{Hunter}
\citation{Hornbostel}
\citation{Olson}
\citation{White}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Aerophones}{36}\protected@file@percent }
\newlabel{subsec-Aerophones}{{3.4}{36}}
\citation{White}
\citation{Olson}
\citation{Olson}
\citation{eqn-OpenClosedCol}
\newlabel{eqn-OpenClosedCol}{{43}{37}}
\citation{Olson}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Spectrograms from woodwind aerophone waveforms\relax }}{38}\protected@file@percent }
\newlabel{fig-PropertiesAerophonesWind}{{10}{38}}
\citation{White}
\citation{White}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Spectrograms from brass aerophone waveforms\relax }}{39}\protected@file@percent }
\newlabel{fig-PropertiesAerophonesBrass}{{11}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Other Generated Sounds}{39}\protected@file@percent }
\newlabel{subsec-Generated}{{3.5}{39}}
\citation{White}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Spectrograms from synethetically generated waveforms\relax }}{40}\protected@file@percent }
\newlabel{fig-PropertiesSynths}{{12}{40}}
\citation{James}
\citation{Loy}
\citation{Serizel}
\citation{Geron}
\citation{James}
\citation{Khan}
\citation{Mierswa}
\citation{Serizel}
\citation{Liu}
\citation{Olson}
\citation{Virtanen}
\citation{Liu}
\citation{Goodfellow}
\citation{James}
\citation{Serizel}
\citation{Goodfellow}
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\@writefile{toc}{\contentsline {section}{\numberline {4}Feature Selections}{42}\protected@file@percent }
\newlabel{sec-Features}{{4}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Feature Space}{42}\protected@file@percent }
\citation{Virtanen}
\citation{Goodfellow}
\citation{James}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{James}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Decision Boundaries}{43}\protected@file@percent }
\newlabel{subsubsec-Decision}{{4.1.1}{43}}
\citation{James}
\citation{Loy}
\citation{Geron}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Feature Space with Linear separability between two classes\relax }}{44}\protected@file@percent }
\newlabel{fig-LinSep1}{{13}{44}}
\citation{Bishop}
\citation{James}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Feature Space with near linear separability between two classes\relax }}{45}\protected@file@percent }
\newlabel{fig-LinSep2}{{14}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Feature Space with near linear non-separability between two classes\relax }}{45}\protected@file@percent }
\newlabel{fig-LinNonSep}{{15}{45}}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{James}
\citation{James}
\citation{Bishop}
\citation{Geron}
\citation{Li}
\citation{Ngiam}
\citation{Tensorflow}
\citation{Loy}
\citation{Geron2}
\citation{James}
\citation{Serizel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}The Design Matrix}{46}\protected@file@percent }
\newlabel{subsubsec-DesignMatrix}{{4.1.2}{46}}
\newlabel{eqn-X1Shape}{{44}{46}}
\newlabel{eqn-X2Shape}{{45}{46}}
\newlabel{eqn-YShape}{{46}{46}}
\citation{Virtanen}
\citation{Philharmonia}
\citation{UnivIowa}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Audio Preprocessing}{47}\protected@file@percent }
\newlabel{subsubsec-Preprocessing}{{4.1.3}{47}}
\citation{Geron}
\citation{Goodfellow}
\citation{Loy}
\citation{Mierswa}
\citation{Liu}
\citation{Zhang}
\citation{Khan}
\citation{Serizel}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Spectrogram Features}{48}\protected@file@percent }
\newlabel{subsec-Spectrogram}{{4.2}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Spectrogram representations of various waveforms. Note that all spectrograms are color-coded according to the log-power spectrum.\relax }}{48}\protected@file@percent }
\newlabel{fig-Spectrograms}{{16}{48}}
\citation{Liu}
\citation{Liu}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Frame Blocking}{49}\protected@file@percent }
\newlabel{subsubsec-FrameBlocking}{{4.2.1}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces A visualization of how frame-blocking is used to create each analysis frames. This image has been adapted and modified from Liu, et. al. "Audio Feature Extraction and Analysis", Fig. (1). See ref. \cite  {Liu}.\relax }}{49}\protected@file@percent }
\newlabel{fig-AnalysisFrames}{{17}{49}}
\newlabel{eqn-FrameMatrix}{{47}{49}}
\newlabel{eqn-IndexingA}{{48}{50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Windowing}{50}\protected@file@percent }
\newlabel{subsubsec-Windowing}{{4.2.2}{50}}
\newlabel{eqn-Hanning}{{49}{50}}
\newlabel{eqn-WindowMatrix}{{50}{50}}
\citation{Virtanen}
\citation{Olson}
\citation{Peatross}
\citation{Virtanen}
\citation{Taylor}
\citation{Peatross}
\citation{Virtanen}
\citation{Short}
\citation{Peatross}
\citation{Levine}
\citation{Loy}
\citation{Tensorflow}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Discrete Fourier Transform}{51}\protected@file@percent }
\newlabel{subsubsec-DFT}{{4.2.3}{51}}
\newlabel{eqn-DFTMatrix}{{51}{51}}
\newlabel{eqn-DFT}{{52}{51}}
\newlabel{eqn-Spectrogram}{{53}{51}}
\citation{Short}
\citation{Peatross}
\citation{Olson}
\citation{Virtanen}
\citation{White}
\newlabel{eqn-IndexingS}{{54}{52}}
\newlabel{eqn-X1}{{55}{52}}
\citation{Serizel}
\citation{Liu}
\citation{Olson}
\citation{Serizel}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Time-Space Features}{53}\protected@file@percent }
\newlabel{subsec-TimeFeatures}{{4.3}{53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Time Domain Envelope}{53}\protected@file@percent }
\citation{Virtanen}
\citation{Olson}
\newlabel{eqn-RMS}{{56}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces TDE Envelope values for musical instruments\relax }}{54}\protected@file@percent }
\newlabel{fig-ExampleTDE}{{19}{54}}
\citation{Khan}
\citation{Liu}
\citation{Zhang}
\citation{Serizel}
\citation{Liu}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces A comparison of the first three of five time domain envelope values across each class using a box-and-whisker plot\relax }}{55}\protected@file@percent }
\newlabel{fig-FeatureTDE}{{20}{55}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Zero Crossing Rate}{55}\protected@file@percent }
\citation{Liu}
\citation{White}
\newlabel{eqn-ZXR}{{57}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces A comparison of the zero-crossing rate for each class using a box-and-whisker plot\relax }}{56}\protected@file@percent }
\newlabel{fig-FeatureZXR}{{21}{56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Temporal Center of Mass}{56}\protected@file@percent }
\newlabel{eqn-FeatureTCM}{{58}{56}}
\citation{Olson}
\citation{White}
\citation{Serizel}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces TCM values for musical instruments\relax }}{57}\protected@file@percent }
\newlabel{fig-ExampleTCM}{{22}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces A comparison of the temporal center mass for each class using a box-and-whisker plot\relax }}{57}\protected@file@percent }
\newlabel{fig-FeatureTCM}{{23}{57}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Auto Correlation Coefficients}{57}\protected@file@percent }
\citation{Serizel}
\newlabel{eqn-FeatureACC}{{59}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces A comparison of the first four auto correlation coefficients in each class using box-and-whisker plots\relax }}{59}\protected@file@percent }
\newlabel{fig-FeatureACC}{{24}{59}}
\citation{Sahidullah}
\citation{Serizel}
\citation{Sahidullah}
\citation{Serizel}
\citation{Serizel}
\citation{Khan}
\citation{Olson}
\citation{Serizel}
\citation{Khan}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Frequency-Space Features}{60}\protected@file@percent }
\newlabel{subsec-FreqFeatures}{{4.4}{60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Mel Filter Bank Energies}{60}\protected@file@percent }
\newlabel{eqn-HztoMel}{{60}{60}}
\newlabel{eqn-MeltoHz}{{61}{60}}
\citation{Serizel}
\citation{Sahidullah}
\citation{Virtanen}
\citation{Serizel}
\citation{Sahidullah}
\citation{Liu}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Mel Filter Banks shown in frequency space with units of Hertz\relax }}{61}\protected@file@percent }
\newlabel{fig-MelFilterBanks}{{25}{61}}
\newlabel{eqn-FilterBanks}{{62}{61}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Mel Frequency Cepstral Coeffecients}{61}\protected@file@percent }
\newlabel{feat-MFCC}{{63}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces A comparison of 4 Mel Frequency Cepstral Coefficients, $1$, $4$, $8$ and $12$ for each class using box-and-whisker plots\relax }}{63}\protected@file@percent }
\newlabel{fig-FeatureMFCCs}{{26}{63}}
\citation{Olson}
\citation{White}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Frequency Center of Mass}{64}\protected@file@percent }
\newlabel{eqn-FeatureFCM}{{64}{64}}
\newlabel{fig-FeatureFCM}{{\caption@xref {fig-FeatureFCM}{ on input line 1695}}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces A comparison of the frequency center of mass for each class using box-and-whisker plots\relax }}{64}\protected@file@percent }
\citation{Geron}
\citation{Goodfellow}
\citation{Geron2}
\citation{Mitchell}
\citation{Geron}
\citation{Geron}
\citation{James}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluating Model Performance}{65}\protected@file@percent }
\newlabel{sec-PerfEval}{{5}{65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}K-Folds Cross Validation}{65}\protected@file@percent }
\newlabel{subsec-XValidation}{{5.1}{65}}
\newlabel{eqn-XValSplit}{{66}{65}}
\citation{James}
\citation{Geron}
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces A $K$-Fold Cross Validation program.\relax }}{66}\protected@file@percent }
\newlabel{alg-CrossValidation}{{7}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Performance Metrics}{66}\protected@file@percent }
\citation{Geron}
\citation{Geron}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Confusion Matrix}{67}\protected@file@percent }
\newlabel{eqn-ConfMat}{{5.2.1}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Example Confusion Matrices for $4$-categories classifier\relax }}{67}\protected@file@percent }
\newlabel{fig-DummyConfMat}{{28}{67}}
\citation{James}
\citation{Loy}
\citation{Geron}
\citation{James}
\citation{Geron}
\citation{James}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Accuracy Score}{68}\protected@file@percent }
\newlabel{eqn-BinaryAccuracy}{{67}{68}}
\citation{Geron}
\citation{James}
\newlabel{eqn-Accuracy}{{68}{69}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Precision Score}{69}\protected@file@percent }
\newlabel{eqn-BinaryPrecision}{{70}{69}}
\newlabel{eqn-KPrecision}{{71}{69}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Recall Score}{69}\protected@file@percent }
\newlabel{eqn-BinaryRecall}{{72}{69}}
\citation{Geron}
\citation{James}
\citation{Geron}
\citation{Geron}
\citation{James}
\citation{Goodfellow}
\citation{Virtanen}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{James}
\citation{Loy}
\newlabel{eqn-KRecall}{{73}{70}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.5}F1-Score}{70}\protected@file@percent }
\newlabel{eqn-F1Score}{{74}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Tracking Metrics over a Period of Training}{70}\protected@file@percent }
\newlabel{subsec-TrainingMetrics}{{5.3}{70}}
\@writefile{toc}{\contentsline {paragraph}{}{70}\protected@file@percent }
\citation{Geron}
\citation{James}
\citation{Geron}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces The loss function score decreases with each training step, indicating that optimization is performing correctly\relax }}{72}\protected@file@percent }
\newlabel{fig-LossScore}{{29}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces The precision score and recall score generally increase with each training step, indicating that the model is getting both more sensitive and more specific as training progresses\relax }}{72}\protected@file@percent }
\newlabel{fig-PrecisionRecallScores}{{30}{72}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experimental Results}{74}\protected@file@percent }
\newlabel{sec-Results}{{6}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Executing Cross Validation}{74}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Performance metrics for the multimodal networks across $10$ models, scores are averaged over $37$ classes\relax }}{74}\protected@file@percent }
\newlabel{fig-MultimodalXval}{{31}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Confusion matrices, each is averaged over 10 folds of cross validation. Recall that each integer represents a class labeled, as given in Tab. (7\hbox {})\relax }}{75}\protected@file@percent }
\newlabel{fig-MultimodalConfs}{{32}{75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Comparing Results between Architectures}{75}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces \relax }}{76}\protected@file@percent }
\newlabel{fig-UnimodalXVal}{{33}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Performance metrics for the unimodal networks across $10$ models, scores are averaged over $37$ classes\relax }}{77}\protected@file@percent }
\newlabel{fig-UnimodalConfs}{{34}{77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Comparing Classification Scores within Each Class}{77}\protected@file@percent }
\newlabel{subsec-ClassScores}{{6.3}{77}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}High Woodwind Scores}{78}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces \relax }}{78}\protected@file@percent }
\newlabel{fig-HighWindsScores}{{35}{78}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Middle and Low Woodwind Scores}{79}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces \relax }}{79}\protected@file@percent }
\newlabel{fig-LowWindsScores}{{36}{79}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3}Brass Scores}{80}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces \relax }}{80}\protected@file@percent }
\newlabel{fig-BrassScores}{{37}{80}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.4}String Scores}{81}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces \relax }}{81}\protected@file@percent }
\newlabel{fig-StringScores}{{38}{81}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.5}Percussion Scores}{82}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces \relax }}{82}\protected@file@percent }
\newlabel{fig-PercussionScores}{{39}{82}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.6}Synthetic Waveform Scores}{83}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces \relax }}{83}\protected@file@percent }
\newlabel{fig-SynthScores}{{40}{83}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Discussion of Results}{83}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{86}\protected@file@percent }
\newlabel{sec-Conclusion}{{7}{86}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgments}{87}\protected@file@percent }
\newlabel{sec-Acknowledge}{{8}{87}}
\bibstyle{apalike}
\bibcite{Bishop}{1}
\bibcite{Geron}{2}
\bibcite{Geron2}{3}
\bibcite{Goodfellow}{4}
\bibcite{Haberman}{5}
\bibcite{Hornbostel}{6}
\bibcite{Hunter}{7}
\bibcite{James}{8}
\bibcite{Khan}{9}
\bibcite{Levine}{10}
\bibcite{Li}{11}
\bibcite{Liu}{12}
\bibcite{Loy}{13}
\bibcite{Matplotlib}{14}
\bibcite{McCulloch}{15}
\bibcite{Mierswa}{16}
\bibcite{Mitchell}{17}
\bibcite{Ngiam}{18}
\bibcite{Numpy}{19}
\bibcite{Olson}{20}
\bibcite{Peatross}{21}
\bibcite{Petrik}{22}
\bibcite{Philharmonia}{23}
\bibcite{Powers}{24}
\bibcite{Sahidullah}{25}
\bibcite{Serizel}{26}
\bibcite{Scipy}{27}
\bibcite{Short}{28}
\bibcite{Sklearn}{29}
\bibcite{Taylor}{30}
\bibcite{Tensorflow}{31}
\bibcite{Virtanen}{32}
