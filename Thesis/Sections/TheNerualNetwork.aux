\relax 
\citation{Geron2}
\citation{Goodfellow}
\citation{Levine}
\citation{Goodfellow}
\citation{James}
\citation{Virtanen}
\citation{Geron}
\citation{Olsen}
\citation{White}
\@writefile{toc}{\contentsline {section}{\numberline {1}The Neural Network}{1}\protected@file@percent }
\newlabel{sec-TheNeuralNetwork}{{1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}An Introduction to Neural Networks}{1}\protected@file@percent }
\newlabel{sec-NerualNetworkIntro}{{1.1}{1}}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{James}
\citation{Loy}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The Structure of a Neural Network}{2}\protected@file@percent }
\newlabel{sec-NetworkStructre}{{1.2}{2}}
\newlabel{eqn-FunctionChain}{{1}{2}}
\newlabel{eqn-LayerFunction}{{2}{2}}
\newlabel{eqn-altLayerFunction}{{3}{2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward propagation system in a standard deep neural network. Each iteration in the main \textit  {for-loop} represents the execution of a layer, and passing the result to the "next" layer function. A practical application of this algorithm should include batches of samples instead of a single sample. \leavevmode {\color  {red}I will put this some place better soon! It is here for future placement}\relax }}{3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algFeedForward}{{1}{3}}
\citation{Goodfellow}
\citation{Geron2}
\citation{Loy}
\citation{McCulloch}
\citation{Geron}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Layers Used in Classification Neural Network}{4}\protected@file@percent }
\newlabel{sec-Layers}{{1.3}{4}}
\newlabel{eqn-matVectEqn}{{4}{4}}
\newlabel{eqn-elementActivation}{{5}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Dense Layer}{4}\protected@file@percent }
\newlabel{subsec-DenseLayer}{{1.3.1}{4}}
\newlabel{layer-DenseNeurons}{{6}{4}}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{eqn-FunctionDense}{{7}{5}}
\newlabel{eqn-DenseFeedForward}{{9}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}2-Dimensional Convolution Layer}{5}\protected@file@percent }
\newlabel{eqn-convolution}{{10}{5}}
\citation{Goodfellow}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Loy}
\newlabel{fig-2DConvExample}{{\caption@xref {fig-2DConvExample}{ on input line 207}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The result of convolving an input (a) with an filter map (b) is a new set of activations (c). This Image was adapted from Goodfellow, pg. 325 \cite  {Goodfellow}\relax }}{6}\protected@file@percent }
\newlabel{eqn-ConvFeedForward}{{12}{6}}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}2-Dimensional Maximum Pooling Layer}{7}\protected@file@percent }
\newlabel{fig-2DMaxPool}{{\caption@xref {fig-2DMaxPool}{ on input line 245}}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The result of 2D maximum-pooling an input array. This image was adapted from Loy, pg. 126 \cite  {Loy}\relax }}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4}1-Dimensional Flattening Layer}{7}\protected@file@percent }
\newlabel{eqn-FlattenFunction}{{13}{7}}
\citation{Geron}
\citation{Loy}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.5}1-Dimensional Concatenation Layer}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Activation Functions Used in Network Layers}{8}\protected@file@percent }
\newlabel{sec-ActivationFunctions}{{1.4}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Rectified Linear Unit}{8}\protected@file@percent }
\newlabel{eqn-ReLU}{{14}{8}}
\newlabel{fig-ReLU}{{\caption@xref {fig-ReLU}{ on input line 295}}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Rectifed Linear Unit (ReLU) activation function\relax }}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Softmax}{8}\protected@file@percent }
\citation{Goodfellow}
\citation{Mitchell}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{James}
\citation{James}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Training a Neural Network}{9}\protected@file@percent }
\newlabel{sec-Training}{{1.5}{9}}
\newlabel{eqn-Theta}{{15}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}The Cost Function}{9}\protected@file@percent }
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Geron}
\citation{Goodfellow}
\citation{James}
\newlabel{eqn-CategoricalCrossentropy}{{16}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}Gradient Based Learning}{10}\protected@file@percent }
\newlabel{eqn-CostGradient}{{17}{10}}
\citation{Geron}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backwards propagation system, in a standard densely connected deep neural network. Each iteration in the \textit  {for-loop} computes the gradient of the cost function $J$ with respect to the weight and bias arrays. Each element in those arrays is then the discrete gradient of that parameter. A practical application of this algorithm should include batches of samples instead of a single sample\relax }}{11}\protected@file@percent }
\newlabel{algBackProp}{{2}{11}}
\citation{Geron}
\citation{Geron}
\citation{Goodfellow}
\citation{Geron}
\newlabel{eqn-GradientLearning}{{18}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.3}The Optimizer}{12}\protected@file@percent }
\newlabel{eqn-ADAMupdate}{{19}{12}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Adaptive-Moments (ADAM) optimizer for a neural network\relax }}{13}\protected@file@percent }
\newlabel{algAdaGrad}{{3}{13}}
\citation{Geron}
\citation{Kahn}
\citation{Liu}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Chosen Model Architecture}{14}\protected@file@percent }
\newlabel{sec-Architecture}{{1.6}{14}}
\citation{White}
\citation{Olsen}
\citation{Kahn}
\newlabel{fig-NetworkArchitecture}{{\caption@xref {fig-NetworkArchitecture}{ on input line 472}}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The developed architecture of the audio file classification neural network. The Left branch process an image-like input, the right branch processes a vector-like input. The activations are then merged, and then a single output is produced\relax }}{15}\protected@file@percent }
\citation{Geron}
\citation{Kahn}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1}The Spectrogram Branch}{16}\protected@file@percent }
\newlabel{eqn-shapeX1}{{20}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.2}The Perceptron Branch}{16}\protected@file@percent }
\newlabel{eqn-shapeX2}{{21}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.3}The Final Output Branch}{17}\protected@file@percent }
\bibstyle{apalike}
\bibcite{Geron}{1}
\bibcite{Geron2}{2}
\bibcite{Goodfellow}{3}
\bibcite{James}{4}
\bibcite{Kahn}{5}
\bibcite{Levine}{6}
\bibcite{Liu}{7}
\bibcite{Loy}{8}
\bibcite{McCulloch}{9}
\bibcite{Mierswa}{10}
\bibcite{Mitchell}{11}
\bibcite{Olsen}{12}
\bibcite{Peatross}{13}
\bibcite{Petrik}{14}
\bibcite{Short}{15}
\bibcite{Virtanen}{16}
\bibcite{White}{17}
\bibcite{Zhang}{18}
