\relax 
\citation{Geron2}
\citation{Goodfellow}
\citation{Levine}
\citation{Goodfellow}
\citation{James}
\citation{Virtanen}
\citation{Geron}
\citation{Olsen}
\citation{White}
\@writefile{toc}{\contentsline {section}{\numberline {1}The Neural Network}{1}\protected@file@percent }
\newlabel{sec-TheNeuralNetwork}{{1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}An Introduction to Neural Networks}{1}\protected@file@percent }
\newlabel{sec-NerualNetworkIntro}{{1.1}{1}}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{James}
\citation{Loy}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The Structure of a Neural Network}{2}\protected@file@percent }
\newlabel{sec-NetworkStructre}{{1.2}{2}}
\newlabel{eqn-FunctionChain}{{1}{2}}
\newlabel{eqn-LayerFunction}{{2}{2}}
\newlabel{eqn-altLayerFunction}{{3}{2}}
\citation{Goodfellow}
\citation{Geron2}
\citation{Loy}
\citation{McCulloch}
\citation{Geron}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Layers Used in Classification Neural Network}{3}\protected@file@percent }
\newlabel{sec-Layers}{{1.3}{3}}
\newlabel{eqn-matVectEqn}{{4}{3}}
\newlabel{eqn-elementActivation}{{5}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Dense Layer}{3}\protected@file@percent }
\newlabel{subsec-DenseLayer}{{1.3.1}{3}}
\newlabel{layer-DenseNeurons}{{6}{3}}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{eqn-FunctionDense}{{7}{4}}
\newlabel{eqn-DenseFeedForward}{{9}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}2-Dimensional Convolution Layer}{4}\protected@file@percent }
\newlabel{eqn-convolution}{{10}{4}}
\citation{Goodfellow}
\citation{Loy}
\citation{Goodfellow}
\citation{Loy}
\citation{Loy}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig-2DConvExample}{{\caption@xref {fig-2DConvExample}{ on input line 168}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The result of convolving an input (a) with an filter map (b) is a new set of activations (c). This Image was adapted from Goodfellow, pg. 325 \cite  {Goodfellow}\relax }}{5}\protected@file@percent }
\newlabel{eqn-ConvFeedForward}{{12}{5}}
\citation{Geron}
\citation{Loy}
\citation{Goodfellow}
\citation{Geron}
\citation{Loy}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}2-Dimensional Maximum Pooling Layer}{6}\protected@file@percent }
\newlabel{fig-2DMaxPool}{{\caption@xref {fig-2DMaxPool}{ on input line 206}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The result of 2D maximum-pooling an input array. This image was adapted from Loy, pg. 126 \cite  {Loy}\relax }}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4}1-Dimensional Flattening Layer}{6}\protected@file@percent }
\newlabel{eqn-FlattenFunction}{{13}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.5}1-Dimensional Concatenation Layer}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Activation Functions Used in Network Layers}{7}\protected@file@percent }
\newlabel{sec-ActivationFunctions}{{1.4}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Rectified Linear Unit}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Softmax}{7}\protected@file@percent }
\citation{Goodfellow}
\citation{Mitchell}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{James}
\citation{James}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Training a Neural Network}{8}\protected@file@percent }
\newlabel{sec-Training}{{1.5}{8}}
\newlabel{eqn-Theta}{{14}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}The Cost Function}{8}\protected@file@percent }
\citation{Goodfellow}
\newlabel{eqn-CategoricalCrossentropy}{{15}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}The Optimizer}{9}\protected@file@percent }
\citation{Geron}
\citation{Kahn}
\citation{Liu}
\citation{Loy}
\citation{Geron}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Chosen Model Architecture}{10}\protected@file@percent }
\newlabel{sec-Architecture}{{1.6}{10}}
\citation{White}
\citation{Olsen}
\citation{Kahn}
\newlabel{fig-NetworkArchitecture}{{\caption@xref {fig-NetworkArchitecture}{ on input line 301}}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The developed architecture of the audio file classification neural network. The Left branch process an image-like input, the right branch processes a vector-like input. The activations are then merged, and then a single output is produced\relax }}{11}\protected@file@percent }
\citation{Geron}
\citation{Kahn}
\citation{Virtanen}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1}The Spectrogram Branch}{12}\protected@file@percent }
\newlabel{eqn-shapeX1}{{16}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.2}The Perceptron Branch}{12}\protected@file@percent }
\newlabel{eqn-shapeX2}{{17}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.3}The Final Output Branch}{13}\protected@file@percent }
\bibstyle{apalike}
\bibcite{Geron}{1}
\bibcite{Geron2}{2}
\bibcite{Goodfellow}{3}
\bibcite{James}{4}
\bibcite{Kahn}{5}
\bibcite{Levine}{6}
\bibcite{Liu}{7}
\bibcite{Loy}{8}
\bibcite{McCulloch}{9}
\bibcite{Mierswa}{10}
\bibcite{Mitchell}{11}
\bibcite{Olsen}{12}
\bibcite{Peatross}{13}
\bibcite{Petrik}{14}
\bibcite{Short}{15}
\bibcite{Virtanen}{16}
\bibcite{White}{17}
\bibcite{Zhang}{18}
