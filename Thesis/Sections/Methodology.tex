% ================================
% Landon Buell
% Kevin Short
% Physics 799%
% 24 Sept 2020 
% ================================


\documentclass[12pt,letterpaper]{article}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{xcolor}

\usepackage[top=2.5cm,left=2.5cm,right=2.5cm]{geometry}


\begin{document}

% ================================================================

\section{Methodology}
\label{sec-Methodology}

\paragraph*{}We outline and detail the steps taken to execute this project from beginning to end.

% ================================================================

\subsection{Designing the Function}

\paragraph*{}Consider the biological process of hearing a sound wave and matching it to a source. We can model this behavior by some unknown function $F$. We produce an approximation of that function $F^*$ that can map the contents of a sound file that have been generated by a chaotic synthesizer to a potential source. For a set of inputs $\vec{x} = \big\{ x_0 , x_1 , x_2 , ... , x_{p-1} \big\}$ and a set of classes $0$ through $k-1$, we denote this function as:
\begin{equation}
\label{eqn-MappingFunction}
F^*: \vec{x} \rightarrow \big\{ 0 , 1 , 2 , ... , k-1 \big\}
\end{equation}


% ================================================================

\subsection{Collecting and Pre-processing Raw Data}

\paragraph*{}In order to train the Multimodal neural network to identify musical instrument sources, we need a suitable data set to present to the model. University of Iowa Electronic Music Studio, and the London-Based Philharmonia Orchestra each have a large collection of publicly available audio files \textcolor{blue}{Citations!}. These contain short segments of musical instruments performing a single note or a collection of notes in succession.

\paragraph*{}To ensure that these data sets are roughly homogeneous, we read each sample from it's original format, \textit{.aif}, \textit{.mp3}, or similar, and rewrite each sample as a new \textit{.wav} files, sampled at 44.1 kHz, with a 16 bit depth \textcolor{red}{check this!}. This ensures that all data will have a consistent format when features are extracted. We also use this stage to ensure that each audio file has a correct label, and to determine the number of unique output classes.

% ================================================================

\subsection{Designing Classification Features}

\paragraph*{}The performance of a neural network is largely dependent of designing an appropriate set of classification features. These are properties of wave forms that can be represented by a numerical value or several numerical values and are used as the primary tool in classification. There are used in place of a full waveform to represent a file's contents We use tools from physics, mathematics, and signal processing to define and explore a comprehensive set of features that enables a high performance of the classifier. 

% ================================================================

\subsection{Designing A Complementary Network Architecture}

\paragraph*{}With the appropriate set of features designed, and the number of output determined, we can organize the structure of the neural network function. We construct a multimodal neural network than processes two input arrays derived from the same audio sample that share a common label. This network is designed to handle and process each respective input independently, and concatenate the results to produce a single output. 

% ================================================================

\subsection{Testing and Evaluating Network Performance}

\paragraph*{}We divide the raw data set up into a training and testing sets. We employ cross-validation and compute the results of performance metrics to ensure that out model is making reliable predictions, and generalize appropriately. In this stage, we also choose the value of hyper-parameters, activation functions and layer widths to best compliment the chosen features. This process is repeated and expanded upon until we have produced a model with a sufficient performance.

% ================================================================

\subsection{Running Predictions of Chaotic Synthesizer Files}

\paragraph*{}Once we have established a suitable performance of the model, we allow the model to run predictions on the un-labeled chaotic synthesizer wave forms. We output the prediction results to a file, and compare the neural network predictions against human predictions. If further corrections are needed, we revert and re-deign the features, architecture, or hyper-parameters as needed.

% ================================================================

\newpage

\begin{thebibliography}{9}
\bibliographystyle{apalike}

\bibitem{Geron}
Geron, Aurelien. \textit{Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems}. O'Reilly, 2017.

\bibitem{Geron2}
Geron, Aurelien. \textit{Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems}. 2nd ed., O'Reilly, 2019.

\bibitem{Goodfellow}
Goodfellow, Ian, et al.\textit{Deep Learning}. MIT Press, 2017.

\bibitem{James}
James, Gareth, et al. \textit{An Introduction to Statistical Learning with Applications in R}. Springer, 2017.

\bibitem{Kahn}
Khan, M. Kashif Saeed, and Wasfi G. Al-Khatib. “Machine-Learning Based Classification of Speech and Music.” Multimedia Systems, vol. 12, no. 1, 2006, pp. 55–67., doi:10.1007/s00530-006-0034-0.

\bibitem{Levine}
Levine, Daniel S. \textit{Introduction to Neural and Cognitive Modeling}. 3rd ed., Routledge, 2019.

\bibitem{Liu}
Liu, Zhu, et al. "Audio Feature Extraction and Analysis for Scene Segmentation and Classification." Journal of VLSI Signal Processing, vol. 20, 1998, pp. 61–79.

\bibitem{Loy}
Loy, James , \textit{Neural Network Projects with Python}. Packt Publishing, 2019

\bibitem{McCulloch}
McCulloch, Warren S., and Walter Pitts. "A Logical Calculus of the Ideas Immanent in Nervous Activity." \textit{The Bulletin of Mathematical Biophysics}, vol. 5, no. 4, 1943, pp. 115–133.

\bibitem{Mierswa}
Mierswa, Ingo, and Katharina Morik. "Automatic Feature Extraction for Classifying Audio Data." \textit{Machine Learning}, vol. 58, no. 2-3, 2005, pp. 127–149., doi:10.1007/s10994-005-5824-7.

\bibitem{Mitchell}
Mitchell, Tom Michael. Machine Learning. 1st ed., McGraw-Hill, 1997.

\bibitem{Olsen}
Olson, Harry E. \textit{Music, Physics and Engineering}. 2nd ed., Dover Publications, 1967.

\bibitem{Peatross}
Peatross, Justin, and Michael Ware. \textit{Physics of Light and Optics.} Brigham Young University, Department of Physics, 2015.

\bibitem{Petrik}
Petrik, Marek. "Introduction to Deep Learning." Machine Learning. 20 April. 2020, Durham, New Hampshire.

\bibitem{Short}
Short, K. and Garcia R.A. 2006. "Signal Analysis Using the Complex Spectral Phase Evolution (CSPE) Method." AES: \textit{Audio Engineering Society Convention Paper}.

\bibitem{Virtanen}
Virtanen, Tuomas, et al. \textit{Computational Analysis of Sound Scenes and Events.} Springer, 2018.

\bibitem{White}
White, Harvey Elliott, and Donald H. White. \textit{Physics and Music: the Science of Musical Sound}. Dover Publications, Inc., 2019.

\bibitem{Zhang}
Zhang, Tong, and C.-C. Jay Kuo. “Content-Based Classification and Retrieval of Audio.” \textit{Advanced Signal Processing Algorithms, Architectures, and Implementations VIII}, 2 Oct. 1998, pp. 432–443., doi:10.1117/12.325703.

\end{thebibliography}

% ================================================================

\end{document}