% ================================
% Landon Buell
% Kevin Short
% Physics 799
% 5 September 2020 
% ================================


\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[top=2.5cm,left=2.5cm,right=2.5cm]{geometry}

\DeclareMathOperator{\sign}{\text{sign}}

\begin{document}

% ================================================================

\section{Feature Selections}
\label{sec-FeatureSelections}

\paragraph*{}Even best minds in the world will perform poorly on test which they have studied the wrong material- neural networks are no different. In order to properly train a neural network, the model must be presented with an appropriate set of training input $x$ and a complementary set of training labels $y$ \cite{Geron,Goodfellow,James}. It becomes quickly apparent that the nature of information contained in the input object $x$, called \textit{features}, is \textit{extremely important} to the performance of the classifier. Consider if you were tasked to identify cats and dogs from images, but instead were presented with only the top-most row of pixels- The task would be nearly impossible because of an inappropriate or in complete set of information.

\paragraph*{}Tuomas Virtanen, machine learning and audio engineer writes in his book, "Computational Analysis of Sound Scene and Events" \cite{Virtanen}:
\begin{quote}
For recognition algorithms, the necessary property of the acoustic features is low variability among features extracted from examples assigned to the same class, and at the same time high variability allowing distinction between features extracted from examples assigned to different classes.
\end{quote}
In constructing a neural network classifier, the development of appropriate features is of the utmost importance. To ensure the construction of a suitable model, we derive features based from three sources (i) a spectrogram matrix of the waveform, (ii) the time-space representation of the waveform, and (iii) the frequency space representation of the waveform. It is important to note that although this algorithm will classify sound waves to instruments, the model will never actually be presented with a waveform directly, instead it will rely on these features.

\paragraph*{}Once we produce an sufficient set of features, we concatenate them into a single object, $\hat{x}$, called the \textit{feature-vector} \cite{Goodfellow}. In the training process, this object, along with the appropriate classification label, $y$ is presented to the neural network for processing. This process of constructing a feature vector from any data set is vital and is used to represent the data set in a far more compact and non-redundant format \cite{Virtanen,Liu}

\paragraph*{}To ensure suitable performance of this sound wave classification neural network, a great deal of time has been devoted to the construction of the elements of the feature vector. These features are derived are principles of music, digital signal processing, previous work success, and most importantly physics. In the following sections, we outline the set of $24$ features used in the classification process

\newpage

\subsubsection{Audio Preprocessing}

\paragraph*{}Preprocessing a data set is a necessary step to execute prior to feature extraction \cite{Geron2,James}. In the case of audio files, preprocessing usually consists of ensuring that the data set contains the following:
\begin{enumerate}
\item A suitably sized number of files, of reasonable audio quality, with normalized ampltudes
\item Audio encoded in a standard, and consistent format
\item A consistent sample rate between audio files
\item A consistent number of channels
\end{enumerate}
Note that different projects may require a different set of requirement from preprocessing\cite{Virtanen}. For this project, we have chosen to use the following parameters:
\begin{enumerate}
\item Roughtly $4000$ audio files Professionally or semi-professionally recorded in a studio. \textcolor{red}{Citation needed!}. All ampltiudes have been normalized to $\pm 1$ unit.
\item All audio has be converted into \textit{.WAV} files from other formats, such as \textit{.AIF} or \textit{.MP3} using a MATLAB program
\item All audio is sampled at $44,100$ Hz
\item All audio has been down-mixed into mono-channel waveforms.
\end{enumerate}

% ================================================================

\newpage

\subsection{Spectrogram Feature}
\label{subsec-spectrogram}

\paragraph*{}The field of neural classification is well studied in the application of image-processing. Many large-scale, and introductory neural network projects find themselves under the umbrella of image classification \cite{Geron,Goodfellow,Loy,Mierswa}. As a result, model architectures for image related tasks are well-explored and have experimentally shown successful behavior. Following in those footsteps, it make senses to provide an image-like representation of a sound wave as a feature. We do this in the form of a spectrogram.

\paragraph*{}A spectrogram is a representation of the energy distribution of a sound wave as a function of both space and time. In a conventional spectrogram, the passing of time is shown along the $x$-axis, and the frequency spectrum is shown on the $y$-axis. Thus each point in the 2-Dimensional space is an energy at a given time and frequency. Examples spectrograms from the wave form data set are shown in Fig. (\ref{fig-spectrograms}).

\begin{figure}[H]
\begin{center}
\label{fig-spectrograms}
\textcolor{red}{Insert spectrograms here}
\caption{Spectrogram representations of various waveforms}
\end{center}
\end{figure}

\paragraph*{}A spectrogram is produced by the method of \textit{frame-blocking}, which is very prevalent in audio signal classification. Frame-blocking creates a set of analysis frames, $a^{(i)}$ each of which is $N$ samples in length, and has a fixed overlap with the next adjacent frame. Each of the $k$ frames then allows for a section of the signal in somewhat stationary state \cite{Liu,Zhan,Kahn,Virtanen}. For this project, we have chosen to use frames of size $N = 4096$ with a $75\%$ or $3072$ sample overlap. At a sample rate of $f_s = 44100$ samples/second, each frame represents a slice of time that is about $0.1$ seconds long. 

\paragraph*{}We concatenate each analysis frame, $a^{(i)}, i \in [0,k-1]$ into a single $k \times N$ matrix, called $A$. Each row is a frame, each column is an index in that frame
\begin{equation}
\label{eqn-FrameMatrix}
A = \big\{ a^{(0)} , a^{(1)} , a^{(2)} , ... , a^{(k-1)} \big\} = 
\begin{bmatrix}
a^{(0)}[0] & a^{(0)}[1] & a^{(0)}[2] & \hdots & a^{(0)}[N-1] \\
a^{(1)}[0] & a^{(1)}[1] & a^{(1)}[2] & \hdots & a^{(1)}[N-1] \\
a^{(2)}[0] & a^{(2)}[1] & a^{(2)}[2] & \hdots & a^{(2)}[N-1] \\
\vdots 		& \vdots 	  & \vdots 		& \ddots & \vdots        \\
a^{(k-1)}[0] & a^{(k-1)}[1] & a^{(k-1)}[2] & \hdots & a^{(k-1)}[N-1] \\
\end{bmatrix}
\end{equation}
We use bracket notation, $[j]$ to indicate that each object is array-like. Many programming languages could also use the following indexing conventions for matrix 
$A$:
\begin{equation}
\label{eqn-IndexingA}
A_{i,j} = a^{(i)}[j] = A[i][j] = A[i,j]
\end{equation}
Note that these all represent equivalent entries.

\paragraph*{}After frame-blocking, we apply a \textit{windowing function} to each frame. A standard \textit{Hann Window} of $N$ samples is generated and reshaped into a $N \times 1$ column-array,$H$. The $n$-th index in a Hann window with $N$ samples is defined:
\begin{equation}
\label{eqn-Hann}
H[n] = \frac{1}{2}\bigg[ 1 - \cos\Big( \frac{2\pi n}{N-1}\Big)\bigg]
\end{equation}
The window function is applied to each analysis frame by computing the dot-product of the Hann window array, $H$ and each row of the analysis frames matrix, $A_{i,:}$. Result is an $k \times N$ array, $\widetilde{A}$:
\begin{equation}
\label{eqn-WindowMatrix}
\widetilde{A} = A \cdot H^T
\end{equation}

\paragraph*{}Finally, we perform a \textit{Discrete Fourier Transform} (DFT) to bring the signal from a time domain into a frequency domain. \cite{Olson,Peatross}. The Discrete Fourier Transform is applied by producing an $N \times N$ \textit{transform matrix}, often noted at $\mathbb{W}$. Let $\omega^k = e^{\frac{-2\pi i}{N}k}$, then the DFT matrix for a time-space containing $N$ samples
\begin{equation}
\label{eqn-DFTMatrix}
\mathbb{W} = \frac{1}{\sqrt{N}}
\begin{bmatrix}
1 & 1 & 1 & 1 & \hdots & 1 \\
1 & \omega		& \omega^2 & \omega^3 & \hdots & \omega^{N-1} \\
1 & \omega^2	& \omega^4 & \omega^6 & \hdots & \omega^{2(N-1)} \\
1 & \omega^3	& \omega^6 & \omega^9 & \hdots & \omega^{3(N-1)} \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
1 & \omega^{N-1} & \omega^{2(N-1)} & \omega^{3(N-1)} & \hdots & \omega^{(N-1)^2}
\end{bmatrix}
\end{equation}
Each column of the matrix is a complex sinusoidal oscillating with an integer number of periods within the $N$-sample length window \cite{Short,Peatross}.The DFT is applied by a taking the matrix -product of $\mathbb{W}$ and $\widetilde{A}^T$. The transpose of $\widetilde{A}$ then makes each analysis frame into a column vector, which gives the appropriate dimension for multiplication.

\paragraph*{}Most standard implementations of neural network require all activations to be real floating-point numbers. Since the the DFT matrix introduce complex values, we compute the square of the element-wise $L2$-norm of the resultant array.
\begin{equation}
\label{eqn-Spectrogram}
S = \big{\lVert} \mathbb{W}\widetilde{A}^T \big{\rVert}_{2}^{2}
\end{equation}
Where $\mathbb{W}$ is the DFT matrix from eqn. (\ref{eqn-DFTMatrix}) and $\widetilde{A}^T$ is the transpose of the analysis frames matrix from eqn. (\ref{eqn-WindowMatrix}). The matrix $S$, is the spectrogram representation of the initial waveform, and now contains $N \times k$ real floating-point numbers. We can index matrix $S$ similarly to that of matrix $A$ in eqn. (\ref{eqn-IndexingA}):
\begin{equation}
\label{eqn-IndexingS}
S_{i,j} = s^{(i)}[j] = S[i][j] = S[i,j]
\end{equation}

\paragraph*{}Each Column of the $S$ matrix is now a single-frame that has been moved into a frequency-space representation, thus there are $k$ columns, just as there were $k$ analysis frames. Given the discretized nature of digital audio, the frequency-space representation is not a continuous function, but rather a column vector, where the frequency has been assigned to one of $N$ bins. Since human hearing extends from about $20$ Hz to around $20$ kHz, audio files, such as music and voice recordings are typically sampled around $44.1$ kHz, to ensure that the full range of audio is held by the recording \cite{Olson,Virtanen}. 

\paragraph*{}However, standard musical instruments seldom extend above $8$ kHz \cite{Olson,Virtanen,White}. This means that when constructing the spectrogram, we will rarely ever see energy present above this frequency at any time, and the $S$ matrix will contain mostly zero, or zero-like entries. As a result of this, we can simply crop each matrix, to display frequencies that are only between $0$ Hz and $8000$ Hz. This makes the input array smaller, and eliminates redundant and non-useful information. The number rows in the $S$ matrix is reduced from $N$ down to $N'$. lastly, each raw audio file may contain a different number of samples to begin with, thus a different number of analysis frames are created. To ensure all sample are a homogeneous size, we assert that each matrix must have exactly $k'$ columns. If $k' < k$, columns of zeros are added to pad the input, and if $k' > k$, columns are removed.

\paragraph*{}Each spectrogram is now $N' \times k' \times 1$ ($1$ representing a single gray-scale pixel channel, as opposed to $3$ channels for RGB inputs) and effectively encodes the energy distribution of the waveform a a function of both time and frequency. The spectrogram is the first feature used in this model. For this classifier, we have chosen $N' = 560$ and $k' = 256$. For training, a mini-batch of $b$ samples are concatenated into a single array object. \textcolor{blue}{See Neural Network section for more details}. For a batch of $b$ samples of $N' \times k' \times 1$ spectrograms, we shape $X_1$ such that:
\begin{equation}
\label{eqn-X1 shape}
X_1 = \big\{ S^{(0)},S^{(1)},S^{(2)}, ... , S^{(b-1)} \big\} \in \mathbb{R}^{(b \times N' \times k' \times 1)}
\end{equation}
This matrix is presented to the \textit{Convolution} branch of the neural network from processing. \textcolor{blue}{See Neural Network section for more details}

% ================================================================

\newpage

\subsection{Time-Space Features}
\label{subsec-time}

\paragraph*{}The features described in this section are derived from time-domain representations of each audio sample. In some cases, this can be from the waveform directly, or my manipulation of rows and columns in the analysis frames matrix $A$ from eqn. (\ref{eqn-FrameMatrix}). For each feature, we detail the physical significance and provide a visualization in feature-space.

\subsubsection{Time Domain Envelope}

\paragraph*{}The time domain envelope (TDE) is a method of determining which parts of the sound wave contains most of the energy of the signal. Generally, we start by computing the RMS-Energy of each frame. The larger the RMS energy of the frame, the larger the amplitude of the waveform in that frame. For frames with smaller amplitude, we conclude that the signal has either not begun, or is decaying. The RMS-Energy of a single analysis-frame $a^{(i)}$ is given by \cite{Olson,Virtanen}
\begin{equation}
\label{eqn-RMS}
\text{RMS}\big[ a^{(i)} \big] = \sqrt{\frac{1}{N} \sum_{j=0}^{N-1}a^{(i)}[j]}
\end{equation}

\paragraph*{}We adapt this feature to \textbf{compute the RMS-Energy of the full waveform}. By doing this, we acquire a rough estimate for the energy of a waveform in the entirety of the audio file \cite{Liu}. For instruments with long sustain or release times, such as strings or undampened mallet percussion, hold a comparably large waveform RMS when compared to those instruments without sustain such as brass or percussion.

\begin{figure}[H]
\label{fig-FeatureTDE}
\begin{center}
\textcolor{red}{PLOT OF TDE FEATURES HERE}
\end{center}
\caption{Time domain envelope visualized in feature-space}
\end{figure}

\subsubsection{Zero Crossing Rate}

\paragraph*{}The zero crossing rate (ZXR) of a signal or frame is use to measure how many times that a signal crosses it's equilibrium point. This can be done per total sound wave, or per unit time, such an per analysis-frame. In differentiating speech and music, it is commonly to use the difference between the frame with the highest ZXR and the lowest ZXR. In some cases, the ZXR can be correlated to frequency as well \cite{Kahn,Zhang}. The ZXR for a frame $a^{(i)}$ is given by \cite{Virtanen,Liu}
\begin{equation}
\label{eqn-ZXR}
\text{ZXR}\big[ a^{(i)} \big] = \frac{1}{2} \sum_{j=1}^{N-1} \big{|} \sign(a^{(i)}[j]) - \sign(a^{(i)}[j-1]) \big{|} 
\end{equation}
Where $\sign(x)$ returns $+1$ if $x > 0$, $-1$ if $x < 0$ and $0$ if $x = 0$.

\paragraph*{}We adapt this feature to \textbf{compute the zero crossing rate for the full waveform}. This provides a rough estimate for the average frequency in the full waveform, and can help discern clean periodic signals (low ZXR) from those that may have more volatile behavior (high ZXR)\cite{Virtanen}.

\begin{figure}[H]
\label{fig-FeatureZXR}
\begin{center}
\textcolor{red}{PLOT OF ZXR FEATURES HERE}
\end{center}
\caption{Zero crossing rate visualized in feature-space}
\end{figure}

\subsubsection{Center of Mass}

\paragraph*{}The temporal center of mass (TCM) of a signal is used to compute where in time the amplitude sort of \textit{bunches up}. We treat the full waveform array, $a$, with $M$ samples as a 1-Dimensional discrete mass distribution. The TCM of that waveform is then given:
\begin{equation}
\label{eqn-FeatureTCM}
\text{TCM}\big[ a \big] = \frac{\sum_{j=0}^{M-1}j a[j]}{\sum_{j=0}^{M-1}a[j]}
\end{equation}
This includes negative amplitude values acting as "negative masses".

\paragraph*{}The TCM turns out to be a very unique feature, and very powerful in classification due to it's variance. For instruments with heavier attacks, we expect the TCM to be a very low value as most of the energy and amplitude is concentrated close to the start of the audio file. For instruments with long sustain times, we expect a very centrally located center of mass, and for instruments with long release times, we expect a very late TCM value.

\begin{figure}[H]
\label{fig-FeatureTCM}
\begin{center}
\textcolor{red}{PLOT OF TCM FEATURES HERE}
\end{center}
\caption{Temporal center-of-mass visualized in feature-space}
\end{figure}

\subsubsection{Statistical Distribution Data}
\textcolor{red}{Honetly, I may remove these from the classifier}

\subsubsection{Auto Correlation Coefficients}

\paragraph*{}Auto correlation coefficients (ACC) are rough estimates of the signal spectral distribution \cite{Virtanen}. We can compute any number of ACC's and their value changed depending on the index chosen. It is common to only compute the first $k$ ACC's. For a full waveform signal $a$, with $M$ samples, the $k$-th ACC (indexed from $1$ to $K$) is given by:
\begin{equation}
\label{eqn-FeatureACC}
\text{ACC}_k\big[ a \big] = \frac{\sum_{j=0}^{M-k-1}a[j]a[j+k]} {\sqrt{\sum_{j=0}^{M-k-1}a^2[j]} \sqrt{\sum_{j=0}^{M-k-1}a^2[j+k]}}
\end{equation}

\begin{figure}[H]
\label{fig-FeatureACC}
\begin{center}
\textcolor{red}{PLOT OF ACC FEATURES HERE}
\end{center}
\caption{First four auto correlation coefficients visualized in feature-space}
\end{figure}



% ================================================================

\newpage

\subsection{Frequency-Space Features}
\label{subsec-frequency}

\paragraph*{}The features described in this section are derived from the frequency-domain representations of each audio sample. Each of these features are exacted from the transpose spectrogram representation, $S^T$, where the creation of $S$ is outlined in sec. (\ref{subsec-spectrogram}) and computed in eqn.(\ref{eqn-Spectrogram}). This is done to return analysis frame into a row-like array, representing frequency space divided into $N'$ bins, with $k'$ analysis frames. For each feature, we detail the physical significance and provide a visualization in feature-space.


\subsubsection{Mel Frequency Cepstral Coefficients}

\paragraph*{}Mel Frequency Ceptstral Coefficients (MFCC's) are a very prolific feature used widly in audio classification tasks \cite{Liu,Virtanen,Zhang}. MFCC's are produced by converting a linear-frequency axis with units such as Hertz into units of Mels, which are design to account for the non-linear perception of frequency in humans. The Hertz to Mel and Mel to Hertz transforms are provided \cite{Virtanen}:
\begin{equation}
\label{eqn-HztoMel}
M_f[h] = 2595 \log_{10}\big(1+ \frac{h}{700}\big)
\end{equation}
\begin{equation}
\label{eqn-MeltoHz}
H_f[m] = 700 \big(10^{(\frac{m}{2595})}-1\big)
\end{equation}
Where $M_f$ is the frequency in units of Mels, given $[h]$, a frequency in Hertz, and $H_f$ is the frequency in Hertz given $[m]$ a frequency in Mels.

\paragraph*{}Mel filters are produced by grouping frequency-space into $R$ overlapping bins called \textit{Mel Filter-banks}. In units of Mels, each filter bank, $R_i$ remains a constant width, but when shown in Hertz, the bins increase logarithmically. 
\begin{figure}[H]
\label{fig-MelFilterBanks}
\begin{center}
\textcolor{red}{Figure of a few Mel Filter Banks goes here}
\end{center}
\caption{Mel Filter Banks shown in frequency space with units of Hertz}
\end{figure}
Each of the $R$ filters is created to be $N'$ samples long, and transformed back into units of Hertz. When applied to an analysis frame in the frequency spectrum, the dot-product between the filter and the spectrum gives an approximation of the energy in that filter bank. Thus, each filter is concatenated into a matrix $M$ of shape $R \times N'$, where each row is a filter. We apply the Mel Filter banks to the spectrogram to create matrix $B$:
\begin{equation}
\label{eqn-FilterBanks}
B = S^T M^T
\end{equation}
Matrix $B$ has shape $k' \times R$.

\paragraph*{}Using $S^T$ ensure that each row of the matrix is an analysis frame in frequency space. Similarly, each column of the matrix $M$ is a appropriately scaled Mel filter-bank. This way, the matrix product in eqn. (\ref{eqn-FilterBanks}) allows that $B_{i,j}$ is the dot product between the $i$-th analysis frame and the $j$-th filter-bank. Finally, we compute the average energy across all $k'$ frames. The resulting $R$ floating-point numbers are approximation of the signal's energy in those particular frequency bands. For this project, we have chosen to use $R = 12$ filter-banks, which are all used as features.

\begin{figure}[H]
\label{fig-FeatureMFCC}
\begin{center}
\textcolor{red}{PLOT OF MFCC FEATURES HERE}
\end{center}
\caption{Mel Frequency Ceptral Coefficients in feature-space}
\end{figure}

\subsubsection{Center of Mass}

\paragraph*{}The frequency center-of-mass (FCM) for an audio file provides a strong representation of how overtones and energy is distributed in the signal's frequency domain. As with the temporal center-of-mass, we treat each row of the $S$ matrix as it's own 1-Dimensional mass distribution, and compute the center of mass of that row. This encodes the FCM for a single analysis frame (in frequency-space) in the waveform. For an frequency-analysis frame, $s^{(i)}$, the FCM is given by:
\begin{equation}
\label{eqn-FeatureFCM}
\text{TCM}_{i}\big[ s^{(i)} \big] = \frac{\sum_{j=0}^{N'-1}j s^{(i)}[j]}{\sum_{j=0}^{N'-1}s^{(i)}[j]}
\end{equation}
We compute the FCM for each of the $k'$ frames, and then average the results. We use the average FCM across $k'$ frames to compute the FCM feature:
\begin{equation}
TCM = \sum_{i=0}^{k'} \text{TCM}_{i}\big[ s^{(i)} \big]
\end{equation}

\paragraph*{}The average FCM gives a strong approximation of the instrument or signal source's range. For example, a flute or violin will have a considerably high FCM value, even in their lower registers. Similarly, basses or tubas will have considerably low FCM values. Given that the standard frequency range of some musical instruments is fixed, it is guaranteed that for any particular instrument, the FCM will consistently remain within certain bounds \cite{Olson,White}.

\begin{figure}[H]
\label{fig-FeatureFCM}
\begin{center}
\textcolor{red}{PLOT OF FCM FEATURES HERE}
\end{center}
\caption{Frequency center-of-mass visualized in feature-space}
\end{figure}

% ================================================================

\subsection{The Design Matrix}


% ================================================================

\newpage

\begin{thebibliography}{9}
\bibliographystyle{apalike}

\bibitem{Geron}
Geron, Aurelien. \textit{Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems}. O'Reilly, 2017.

\bibitem{Geron2}
Geron, Aurelien. \textit{Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems}. 2nd ed., O'Reilly, 2019.

\bibitem{Goodfellow}
Goodfellow, Ian, et al.\textit{Deep Learning}. MIT Press, 2017.

\bibitem{James}
James, Gareth, et al. \textit{An Introduction to Statistical Learning with Applications in R}. Springer, 2017.

\bibitem{Kahn}
Khan, M. Kashif Saeed, and Wasfi G. Al-Khatib. “Machine-Learning Based Classification of Speech and Music.” Multimedia Systems, vol. 12, no. 1, 2006, pp. 55–67., doi:10.1007/s00530-006-0034-0.

\bibitem{Levine}
Levine, Daniel S. \textit{Introduction to Neural and Cognitive Modeling}. 3rd ed., Routledge, 2019.

\bibitem{Liu}
Liu, Zhu, et al. "Audio Feature Extraction and Analysis for Scene Segmentation and Classification." Journal of VLSI Signal Processing, vol. 20, 1998, pp. 61–79.

\bibitem{Loy}
Loy, James , \textit{Neural Network Projects with Python}. Packt Publishing, 2019

\bibitem{McCulloch}
McCulloch, Warren S., and Walter Pitts. "A Logical Calculus of the Ideas Immanent in Nervous Activity." \textit{The Bulletin of Mathematical Biophysics}, vol. 5, no. 4, 1943, pp. 115–133.

\bibitem{Mierswa}
Mierswa, Ingo, and Katharina Morik. "Automatic Feature Extraction for Classifying Audio Data." \textit{Machine Learning}, vol. 58, no. 2-3, 2005, pp. 127–149., doi:10.1007/s10994-005-5824-7.

\bibitem{Mitchell}
Mitchell, Tom Michael. Machine Learning. 1st ed., McGraw-Hill, 1997.

\bibitem{Olson}
Olson, Harry E. \textit{Music, Physics and Engineering}. 2nd ed., Dover Publications, 1967.

\bibitem{Peatross}
Peatross, Justin, and Michael Ware. \textit{Physics of Light and Optics.} Brigham Young University, Department of Physics, 2015.

\bibitem{Petrik}
Petrik, Marek. "Introduction to Deep Learning." Machine Learning. 20 April. 2020, Durham, New Hampshire.

\bibitem{Short}
Short, K. and Garcia R.A. 2006. "Signal Analysis Using the Complex Spectral Phase Evolution (CSPE) Method." AES: \textit{Audio Engineering Society Convention Paper}.

\bibitem{Virtanen}
Virtanen, Tuomas, et al. \textit{Computational Analysis of Sound Scenes and Events.} Springer, 2018.

\bibitem{White}
White, Harvey Elliott, and Donald H. White. \textit{Physics and Music: the Science of Musical Sound}. Dover Publications, Inc., 2019.

\bibitem{Zhang}
Zhang, Tong, and C.-C. Jay Kuo. “Content-Based Classification and Retrieval of Audio.” \textit{Advanced Signal Processing Algorithms, Architectures, and Implementations VIII}, 2 Oct. 1998, pp. 432–443., doi:10.1117/12.325703.

\end{thebibliography}

% ================================================================

\end{document}