% ================================
% Landon Buell
% Kevin Short
% Physics 799%
% 24 Sept 2020 
% ================================


\documentclass[12pt,letterpaper]{article}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{xcolor}

\usepackage[top=2.5cm,left=2.5cm,right=2.5cm]{geometry}


\begin{document}

% ================================================================

\section{Evaluating Performance}
\label{sec-PerfEval}

\paragraph*{}Before making predictions on unlabeled data such as the Chaotic Synthesizers, we must determine that our model performs reasonably on data that it has never interacted with. The most common practice is to divide a full data set into a subset of \textit{training} samples, and \textit{testing} samples. The exact ratio of sample volume between these subsets varies depending on the task \cite{Goodfellow,Geron2,Mitchell}. We as the names imply, the training subset is used to fit the model, and we use the labeled testing data set to evaluate. Since the testing data is labeled, we can compare the classifiers predictions to the \textit{ground truth} labels.

% ================================================================

\subsection{Cross Validation}

\paragraph*{}Performance evaluations are most commonly implemented in the form of \textit{$K$-Fold Cross-Validation} (Also called X-val). This process involves taking a data set containing $N$ unique samples and dividing it into $K$ non-overlapping subsets. $1$ subset is reserved to evaluate the model, and $K-1$ are used to train the model. This belongs a larger family of statisical validations called \textit{Resampling methods} \cite{James}. Below, we detail pseudo-code for a $K$-Fold Cross Validation algorithm.

\begin{algorithm}[H]
\caption{A $K$-Fold Cross Validation program.}
\label{algFeedForward}

\begin{algorithmic}
\REQUIRE Untrained Network or related learning algorithm, $F^*$
\REQUIRE A full data set of $N$ samples. $X^{(i)}$, $i \in {0,1,2,...,N-1}$
\REQUIRE Number of splits in Cross validation, $K$
\REQUIRE Performance metric function(s), $P$

Divide Data into $K$ non-overlapping subsets $x_{i}$, each with roughly $K/N$ samples \\
$X \rightarrow \big\{ x_{0}, x_{1}, x_{2}, ..., x_{K-2},  x_{K-1} \big\}$ \\
Performance History $\leftarrow \{\}$

\FOR {$j = 0,1,2,3,...,K-2,K-1$}
	\item Reset all parameters in $F^*$ to a random "untrained" state
	\item Set aside testing data subset
	\item $X_{test} = x_{j}$
	\item Concatenate the remaining subsets into training data set
	\item $X_{train} = x_{j \neq k}$
	\item Train the model, $F^*$ with the $X_{train}$ data set
	\item Evaluate the trained model with metric function(s) $P$
	\item Store Performance $P$ in Performance History array
\ENDFOR

Compare performance results, and adjust model, parameters as needed, and repeat if desired.

\end{algorithmic}
\end{algorithm}

\paragraph*{}By using cross-validation, we gain a deeper understanding of the model to ensure that the model can be properly trained consistently. This also helps counteract the possibility that the model got very lucky or unlucky with a particular set of initial conditions \cite{Geron,James}.



% ================================================================

\subsection{Performance Metrics}

\paragraph*{}In the case of the multi-category classifier, it is also important that we choose the appropriate metrics to evaluate our performance. While the neural network itself uses the cost function as it's sole performance metric to optimize, we also require a set of more human-readable functions. We introduce a set of functions and metrics than enable a more tangible interpretation of model performance. 
To evaluate a performance metric, we require a set of samples with ground truth labels, $y$, and a model's prediction for those labels, $y^*$.

% ================================

\subsubsection{Confusion Matrix}

\paragraph*{}The confusion matrix (also called a confusion table) is a very quick, often graphical model that can be used to show how a model performs over a subset of predictions. The general idea of this object is count the number of times class $A$ is predicted to be in class $B$, and vice-versa \cite{Geron}. If we see that these classes are being repeatedly \textit{confused} in the model's prediction process, then we must modify the model or features to account for it. 

\paragraph*{}For a $k$-classes classifier, a confusion matrix will have shape $k \times k$. Each row represents the actual labeled class and each column represents a predicted class. Thus for a confusion matrix, $C$, we can say that:
\begin{equation}
\label{eqn-ConfMat}
C_{i,j} = 
\text{Number of samples that belong to class $i$, and were predicted to be in class $j$}
\end{equation} 
Thus, $i = j$, represents a correct prediction, and $i \neq k$ indicates an incorrect prediction. A confusion matrix with a strong main diagonal indicates a model that consistently predicts correct labels \cite{Geron}. 

\begin{figure}[H]
\label{fig-ConfMat}
\begin{center}
\textcolor{red}{Images of Confusion Matrices Go Here!} 
\end{center}
\caption{Example Confusion Matrices for $k$-classes tasks}
\end{figure}

\paragraph*{}We can also use a confusion matrix to define four useful quantities relating a prediction to it's label. Suppose a prediction, $y^* = p$ and a truth, $y = q$.
\begin{enumerate}
\item \textbf{True-Positive}\\
A sample is a true-positive if $p = q$
\item \textbf{True-Negative}\\

\item \textbf{False-Positive}\\

\item \textbf{False-Negative}\\
\end{enumerate}
\textcolor{red}{finish this!}

% ================================

\subsubsection{Precision Score}

\paragraph*{}Precision score (also called \textit{specificity}) offers a more concise performance metric than a confusion metric. For a classifier with $k = 2$ unique classes, we define the precision score of a model as:
\begin{equation}
\label{eqn-BinaryPrecision}
\text{Prec} = \frac{TP}{TP + FP}
\end{equation}
Where $TP$ is the number of \textit{true-positive}, and $FP$ is the number of false-positives predictions. For a $k$-classes confusion matrix, $C$, the precision score is given by the entry $C_{j,j}$ divided by the sum of column $j$:
\begin{equation}
\label{eqn-KPrecision}
\text{Prec}_j = \frac{C_{j,j}}{\sum_{i=0}^{k-1}C_{i,j}}
\end{equation}
For multi-class problems, the precision score is usually used as an average over all classes. 

\paragraph*{}
\textcolor{red}{Physical Significance of Precision}

% ================================

\subsubsection{Recall Score}

\paragraph*{}Recall score (also called \textit{sensitivity}) also offers a more concise performance metric than a confusion metric. For a classifier with $k = 2$ unique classes, we define the recall score of a model as:
\begin{equation}
\label{eqn-BinaryRecall}
\text{Recall} = \frac{TP}{TP + FN}
\end{equation}
Where $TP$ is the number of \textit{true-positive}, and $FN$ is the number of false-negative predictions. For a $k$-classes confusion matrix, $C$, the precision score is given by the entry $C_{j,j}$ divided by the sum of row $j$:
\begin{equation}
\label{eqn-KRecall}
\text{Recall}j = \frac{C_{j,j}}{\sum_{i=0}^{k-1}C_{j,i}}
\end{equation}
For multi-class problems, the recall score is also used as an average over all classes. 

\paragraph*{}
\textcolor{red}{Physical Significance of Recall}

% ================================




% ================================================================


% ================================================================



% ================================================================



% ================================================================



% ================================================================



% ================================================================

\newpage

\begin{thebibliography}{9}
\bibliographystyle{apalike}

\bibitem{Geron}
Geron, Aurelien. \textit{Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems}. O'Reilly, 2017.

\bibitem{Geron2}
Geron, Aurelien. \textit{Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems}. 2nd ed., O'Reilly, 2019.

\bibitem{Goodfellow}
Goodfellow, Ian, et al.\textit{Deep Learning}. MIT Press, 2017.

\bibitem{James}
James, Gareth, et al. \textit{An Introduction to Statistical Learning with Applications in R}. Springer, 2017.

\bibitem{Kahn}
Khan, M. Kashif Saeed, and Wasfi G. Al-Khatib. “Machine-Learning Based Classification of Speech and Music.” Multimedia Systems, vol. 12, no. 1, 2006, pp. 55–67., doi:10.1007/s00530-006-0034-0.

\bibitem{Levine}
Levine, Daniel S. \textit{Introduction to Neural and Cognitive Modeling}. 3rd ed., Routledge, 2019.

\bibitem{Liu}
Liu, Zhu, et al. "Audio Feature Extraction and Analysis for Scene Segmentation and Classification." Journal of VLSI Signal Processing, vol. 20, 1998, pp. 61–79.

\bibitem{Loy}
Loy, James , \textit{Neural Network Projects with Python}. Packt Publishing, 2019

\bibitem{McCulloch}
McCulloch, Warren S., and Walter Pitts. "A Logical Calculus of the Ideas Immanent in Nervous Activity." \textit{The Bulletin of Mathematical Biophysics}, vol. 5, no. 4, 1943, pp. 115–133.

\bibitem{Mierswa}
Mierswa, Ingo, and Katharina Morik. "Automatic Feature Extraction for Classifying Audio Data." \textit{Machine Learning}, vol. 58, no. 2-3, 2005, pp. 127–149., doi:10.1007/s10994-005-5824-7.

\bibitem{Mitchell}
Mitchell, Tom Michael. Machine Learning. 1st ed., McGraw-Hill, 1997.

\bibitem{Olsen}
Olson, Harry E. \textit{Music, Physics and Engineering}. 2nd ed., Dover Publications, 1967.

\bibitem{Peatross}
Peatross, Justin, and Michael Ware. \textit{Physics of Light and Optics.} Brigham Young University, Department of Physics, 2015.

\bibitem{Petrik}
Petrik, Marek. "Introduction to Deep Learning." Machine Learning. 20 April. 2020, Durham, New Hampshire.

\bibitem{Short}
Short, K. and Garcia R.A. 2006. "Signal Analysis Using the Complex Spectral Phase Evolution (CSPE) Method." AES: \textit{Audio Engineering Society Convention Paper}.

\bibitem{Virtanen}
Virtanen, Tuomas, et al. \textit{Computational Analysis of Sound Scenes and Events.} Springer, 2018.

\bibitem{White}
White, Harvey Elliott, and Donald H. White. \textit{Physics and Music: the Science of Musical Sound}. Dover Publications, Inc., 2019.

\bibitem{Zhang}
Zhang, Tong, and C.-C. Jay Kuo. “Content-Based Classification and Retrieval of Audio.” \textit{Advanced Signal Processing Algorithms, Architectures, and Implementations VIII}, 2 Oct. 1998, pp. 432–443., doi:10.1117/12.325703.

\end{thebibliography}

% ================================================================

\end{document}